{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8976e1b4",
   "metadata": {},
   "source": [
    "# CNCF and Apache Ecosystem (culled from gharchive.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a81d4",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e79e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import gzip\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1024)\n",
    "pd.set_option('display.max_columns', 512)\n",
    "pd.set_option('display.width', 1024)\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import adbc_driver_postgresql.dbapi\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import panel as pn\n",
    "import altair as alt    # https://altair-viz.github.io/\n",
    "import vegafusion as vf # https://vegafusion.io/\n",
    "\n",
    "import simdjson\n",
    "\n",
    "from notebook_utils import *\n",
    "#from arrow_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0918cc8",
   "metadata": {},
   "source": [
    "## Load Data from Parquet\n",
    "\n",
    "_gharchive.org data has been processed to datasets, one Apache Arrow [Dataset](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Dataset.html) object per [GitHub Event Type](https://docs.github.com/en/rest/using-the-rest-api/github-event-types?apiVersion=2022-11-28)_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd074a8",
   "metadata": {},
   "source": [
    "### JSON Dataset (per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71112498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PullRequestEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/PullRequestEvent.parquet'),\n",
       " 'PushEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/PushEvent.parquet'),\n",
       " 'WatchEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/WatchEvent.parquet'),\n",
       " 'IssuesEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/IssuesEvent.parquet'),\n",
       " 'ForkEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/ForkEvent.parquet'),\n",
       " 'CreateEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/CreateEvent.parquet'),\n",
       " 'PublicEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/PublicEvent.parquet'),\n",
       " 'GollumEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/GollumEvent.parquet'),\n",
       " 'ReleaseEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/ReleaseEvent.parquet'),\n",
       " 'CommitCommentEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/CommitCommentEvent.parquet'),\n",
       " 'PullRequestReviewCommentEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/PullRequestReviewCommentEvent.parquet'),\n",
       " 'PullRequestReviewEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/PullRequestReviewEvent.parquet'),\n",
       " 'IssueCommentEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/IssueCommentEvent.parquet'),\n",
       " 'DeleteEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/DeleteEvent.parquet'),\n",
       " 'MemberEvent': PosixPath('/Users/matt/gharchive-cncf/cncf-parquet-consolidated/MemberEvent.parquet')}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASETS_ROOT = Path(\"~/gharchive-cncf/cncf-parquet-consolidated\").expanduser()\n",
    "DATASET_PATHS = {}\n",
    "\n",
    "for file in DATASETS_ROOT.glob(\"*.parquet\"):\n",
    "    DATASET_PATHS[file.stem] = file\n",
    "display(DATASET_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6196c68d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_parquet_dataset(name: str, path: str) -> pq.ParquetDataset:\n",
    "    #print(f'Loading dataset: {name} from {path}')\n",
    "    pqds = pq.ParquetDataset(path, memory_map=True)\n",
    "    return pqds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f6de377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PullRequestEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x10411fad0>,\n",
       " 'PushEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c576ed0>,\n",
       " 'WatchEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c5763d0>,\n",
       " 'IssuesEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577710>,\n",
       " 'ForkEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577390>,\n",
       " 'CreateEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577790>,\n",
       " 'PublicEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c5777d0>,\n",
       " 'GollumEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577750>,\n",
       " 'ReleaseEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c576650>,\n",
       " 'CommitCommentEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577810>,\n",
       " 'PullRequestReviewCommentEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577590>,\n",
       " 'PullRequestReviewEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577650>,\n",
       " 'IssueCommentEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577850>,\n",
       " 'DeleteEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c5778d0>,\n",
       " 'MemberEvent': <pyarrow.parquet.core._ParquetDatasetV2 at 0x16c577950>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASETS = {}\n",
    "DATASET_SCHEMAS = {}\n",
    "\n",
    "for name, path in DATASET_PATHS.items():\n",
    "    DATASETS[name] = load_parquet_dataset(name, path)\n",
    "\n",
    "for name, dataset in DATASETS.items():\n",
    "    DATASET_SCHEMAS[name] = dataset.schema\n",
    "\n",
    "display(DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d7e55",
   "metadata": {},
   "source": [
    "### Persist Event Schema to files (.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e6e560f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "schema_summaries = {}\n",
    "\n",
    "def dataset_schema_summary(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a summary DataFrame for all fragments in a dataset, including schema details\n",
    "    and partition information.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: The file system path to the dataset.\n",
    "\n",
    "    Returns:\n",
    "    A pandas DataFrame with columns for each schema field, including fragment and partition keys.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(dataset_path, format=\"parquet\")  # Adjust format as needed\n",
    "    summary = []\n",
    "\n",
    "    for fragment in dataset.get_fragments():\n",
    "        schema = fragment.physical_schema\n",
    "        \n",
    "        for field in schema:\n",
    "            summary.append({\n",
    "                \"Fragment\": fragment.path,\n",
    "                \"Field Name\": field.name,\n",
    "                \"Type\": str(field.type),\n",
    "                \"Nullable\": field.nullable\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the summary list\n",
    "    df = pd.DataFrame(summary)\n",
    "    print(f'{df.shape}')\n",
    "    df.drop_duplicates(inplace=True, subset=[\"Field Name\", \"Type\", \"Nullable\"])\n",
    "    print(f'{df.shape}')\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_table(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, table: pa.Table):\n",
    "    \"\"\"\n",
    "    Replace the contents of a specified table with new data from an Arrow Table, preserving the table's schema and indexes.\n",
    "\n",
    "    1. create: temporary table with the new data\n",
    "    2. rename: existing table --> a temporary name\n",
    "    3. rename: new table      --> original table name\n",
    "    4. drop the old table\n",
    "\n",
    "    Parameters:\n",
    "    - conn (adbc_driver_postgresql.dbapi.Connection): A connection to the PostgreSQL database.\n",
    "    - table_name (str): The name of the table to be replaced.\n",
    "    - table (pa.Table): An Arrow Table containing the new data to replace the existing table's contents.\n",
    "\n",
    "    TODO: Validate / tests needed:\n",
    "    \n",
    "    - If an error occurs during the process, transaction rolled back.\n",
    "    - The renaming of the table does not change the table's schema or indexes. The indexes will retain their\n",
    "      original names and continue to reference the renamed table.\n",
    "    \"\"\"\n",
    "    temp_table_name = f\"{table_name}_temp\"\n",
    "    old_table_name = f\"{table_name}_old\"\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.adbc_ingest(temp_table_name, table, mode=\"create\", temporary=True)\n",
    "            cur.execute(f\"ALTER TABLE {table_name} RENAME TO {old_table_name}\")\n",
    "            cur.execute(f\"ALTER TABLE {temp_table_name} RENAME TO {table_name}\")\n",
    "            cur.execute(f\"DROP TABLE {old_table_name}\")\n",
    "    except adbc_driver_postgresql.dbapi.Error as e:\n",
    "        # The transaction is automatically rolled back by the ADBC framework if an error occurs\n",
    "        # TODO: Need to validate here...adbc docs are a mixture of spec and \"use the source luke\" :)\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8946256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# GitHub Archive (https://gharchive.org) Schema Documentation\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "| Field Name  | Type                          |\n",
      "| ----------- | ----------------------------- |\n",
      "| some_int    | int32                         |\n",
      "| some_string | string                        |\n",
      "| some_struct | struct<f1: int32, f2: string> |\n",
      "|             | f1 | int32  |\n",
      "|             | f2 | string |\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "- [some_int](#some_int)\n",
      "- [some_string](#some_string)\n",
      "- [some_struct](#some_struct)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_markdown_documentation(schema: pa.Schema, title: str = \"GitHub Archive (https://gharchive.org) Schema Documentation\") -> str:\n",
    "    \"\"\"\n",
    "    Generates Markdown documentation for a PyArrow Schema object, formatted for GitHub with improved table formatting.\n",
    "\n",
    "    Parameters:\n",
    "    - schema: The PyArrow Schema object to document.\n",
    "    - title: The title of the documentation. Default is \"Schema Documentation\".\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the Markdown documentation.\n",
    "    \"\"\"\n",
    "    markdown = f\"# {title}\\n\\n\"\n",
    "    markdown += \"## Table of Contents\\n\\n\"\n",
    "    toc = []\n",
    "\n",
    "    # Function to calculate padding for each column\n",
    "    def calculate_padding(fields):\n",
    "        max_name_length = max((len(field.name) for field in fields), default=0)\n",
    "        max_type_length = max((len(str(field.type)) for field in fields), default=0)\n",
    "        return max_name_length, max_type_length\n",
    "\n",
    "    # Calculate padding for the top-level fields\n",
    "    max_name_length, max_type_length = calculate_padding(schema)\n",
    "\n",
    "    # Start the table\n",
    "    markdown += f\"| {'Field Name'.ljust(max_name_length)} | {'Type'.ljust(max_type_length)} |\\n\"\n",
    "    markdown += f\"| {'-'*max_name_length} | {'-'*max_type_length} |\\n\"\n",
    "\n",
    "    for field in schema:\n",
    "        field_name = field.name.ljust(max_name_length)\n",
    "        field_type = str(field.type).ljust(max_type_length)\n",
    "        markdown += f\"| {field_name} | {field_type} |\\n\"\n",
    "        if isinstance(field.type, pa.StructType):\n",
    "            # Calculate padding for nested fields\n",
    "            max_nested_name_length, max_nested_type_length = calculate_padding(field.type)\n",
    "            for sub_field in field.type:\n",
    "                nested_name = sub_field.name.ljust(max_nested_name_length)\n",
    "                nested_type = str(sub_field.type).ljust(max_nested_type_length)\n",
    "                markdown += f\"| {' '*max_name_length} | {nested_name} | {nested_type} |\\n\"\n",
    "        toc.append(f\"- [{field.name}](#{field.name.lower().replace(' ', '-')})\\n\")\n",
    "\n",
    "    markdown += \"\\n## Table of Contents\\n\\n\" + \"\".join(toc) + \"\\n\\n\"\n",
    "    return markdown\n",
    "\n",
    "# note: pyarrow.schema() is a factory function that returns a pyarrow.Schema object\n",
    "schema = pa.schema([\n",
    "    ('some_int', pa.int32()),\n",
    "    ('some_string', pa.string()),\n",
    "    ('some_struct', pa.struct([\n",
    "        ('f1', pa.int32()),\n",
    "        ('f2', pa.string())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "print(generate_markdown_documentation(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb730ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_schema_prefix(name: str) -> str:\n",
    "    \"\"\"\n",
    "    if prefix found, remove it from the name used for schema file\n",
    "\n",
    "    names cam be {event_type}.schema, or '{prefix}-{event_type}.schema'\n",
    "    \"\"\"\n",
    "    parts = name.split('-', 1)\n",
    "    if len(parts) > 1:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbfdd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_markdown_docs(schemas: Dict[str, pa.Schema], output_dir: str):\n",
    "    \"\"\"\n",
    "    Generates Markdown documentation for each schema in the provided dictionary and saves them to files.\n",
    "\n",
    "    Parameters:\n",
    "    - schemas: A dictionary mapping event types to PyArrow schema objects.\n",
    "    - output_dir: The directory where the Markdown files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over the schemas and generate Markdown documentation\n",
    "    for event_type, schema in schemas.items():\n",
    "        # Generate the Markdown documentation\n",
    "        markdown_doc = generate_markdown_documentation(schema, title=event_type)\n",
    "\n",
    "        # Save the Markdown documentation to a file\n",
    "        filename = f\"{strip_schema_prefix(event_type)}.md\"\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            f.write(markdown_doc)\n",
    "\n",
    "schemas = {\n",
    "    'IssuesEvent': pa.schema([\n",
    "        ('actor', pa.struct([\n",
    "            ('avatar_url', pa.string()),\n",
    "            ('display_login', pa.string()),\n",
    "            ('gravatar_id', pa.string()),\n",
    "            ('id', pa.int64()),\n",
    "            ('login', pa.string()),\n",
    "            ('url', pa.string())\n",
    "        ])),\n",
    "        # ... other fields ...\n",
    "    ]),\n",
    "    # ... other event types ...\n",
    "}\n",
    "\n",
    "# example\n",
    "create_and_save_markdown_docs(schemas, output_dir='./test-docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e259f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# IssuesEvent\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "| Field Name | Type                                                                                                          |\n",
      "| ----- | ------------------------------------------------------------------------------------------------------------- |\n",
      "| actor | struct<avatar_url: string, display_login: string, gravatar_id: string, id: int64, login: string, url: string> |\n",
      "|       | avatar_url    | string |\n",
      "|       | display_login | string |\n",
      "|       | gravatar_id   | string |\n",
      "|       | id            | int64  |\n",
      "|       | login         | string |\n",
      "|       | url           | string |\n",
      "\n",
      "## Table of Contents\n",
      "\n",
      "- [actor](#actor)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat ./test-docs/IssuesEvent.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d01e5",
   "metadata": {},
   "source": [
    "### Create .schema files and documentation\n",
    "\n",
    "Regen the schema files from the parquet dataset.  As they are commited to [./docs/schemas](./docs/schemas), it's easy to see any deviation and/or changes to the schema are found in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77681d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset                       | schema_file                                         | nbytes\n",
      "----------------------------- | --------------------------------------------------- | -----\n",
      "PullRequestEvent              | ./docs/schemas/PullRequestEvent.schema              | 26755\n",
      "PushEvent                     | ./docs/schemas/PushEvent.schema                     |  1107\n",
      "WatchEvent                    | ./docs/schemas/WatchEvent.schema                    |   728\n",
      "IssuesEvent                   | ./docs/schemas/IssuesEvent.schema                   |  9883\n",
      "ForkEvent                     | ./docs/schemas/ForkEvent.schema                     |  4280\n",
      "CreateEvent                   | ./docs/schemas/CreateEvent.schema                   |   565\n",
      "PublicEvent                   | ./docs/schemas/PublicEvent.schema                   |   433\n",
      "GollumEvent                   | ./docs/schemas/GollumEvent.schema                   |   869\n",
      "ReleaseEvent                  | ./docs/schemas/ReleaseEvent.schema                  |  5463\n",
      "CommitCommentEvent            | ./docs/schemas/CommitCommentEvent.schema            |  1936\n",
      "PullRequestReviewCommentEvent | ./docs/schemas/PullRequestReviewCommentEvent.schema | 27244\n",
      "PullRequestReviewEvent        | ./docs/schemas/PullRequestReviewEvent.schema        | 25703\n",
      "IssueCommentEvent             | ./docs/schemas/IssueCommentEvent.schema             | 17306\n",
      "DeleteEvent                   | ./docs/schemas/DeleteEvent.schema                   |   506\n",
      "MemberEvent                   | ./docs/schemas/MemberEvent.schema                   |  1064\n"
     ]
    }
   ],
   "source": [
    "print(f'{\"dataset\":<29} | {\"schema_file\":<51} | {\"nbytes\":>5}')\n",
    "print(f'{\"\":-<29} | {\"\":-<51} | {\"\":->5}')\n",
    "  \n",
    "for name, dataset in DATASETS.items():\n",
    "    schema_file = f'./docs/schemas/{name}.schema'\n",
    "    with open(schema_file, 'w') as f:\n",
    "        nbytes = f.write(str(dataset.schema))\n",
    "    print(f'{name:<29} | {schema_file:<51} | {nbytes:>5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc7ab",
   "metadata": {},
   "source": [
    "### Generate Documentation\n",
    "\n",
    "**Help Wanted!** Make this better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b1fcb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['PullRequestEvent', 'PushEvent', 'WatchEvent', 'IssuesEvent', 'ForkEvent', 'CreateEvent', 'PublicEvent', 'GollumEvent', 'ReleaseEvent', 'CommitCommentEvent', 'PullRequestReviewCommentEvent', 'PullRequestReviewEvent', 'IssueCommentEvent', 'DeleteEvent', 'MemberEvent'])\n",
      "./docs/github-event-types/CommitCommentEvent.md\n",
      "./docs/github-event-types/CreateEvent.md\n",
      "./docs/github-event-types/DeleteEvent.md\n",
      "./docs/github-event-types/ForkEvent.md\n",
      "./docs/github-event-types/GollumEvent.md\n",
      "./docs/github-event-types/IssueCommentEvent.md\n",
      "./docs/github-event-types/IssuesEvent.md\n",
      "./docs/github-event-types/MemberEvent.md\n",
      "./docs/github-event-types/PublicEvent.md\n",
      "./docs/github-event-types/PullRequestEvent.md\n",
      "./docs/github-event-types/PullRequestReviewCommentEvent.md\n",
      "./docs/github-event-types/PullRequestReviewEvent.md\n",
      "./docs/github-event-types/PushEvent.md\n",
      "./docs/github-event-types/ReleaseEvent.md\n",
      "./docs/github-event-types/WatchEvent.md\n"
     ]
    }
   ],
   "source": [
    "print(DATASET_SCHEMAS.keys())\n",
    "\n",
    "doc_dir = f'./docs/github-event-types'\n",
    "create_and_save_markdown_docs(DATASET_SCHEMAS, doc_dir)\n",
    "!ls -w $doc_dir/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db4e9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BatchResult:\n",
    "    table_name: str\n",
    "    hostname_db: str\n",
    "    ingest_mode: str\n",
    "    rows: int\n",
    "    duration: float\n",
    "    rows_per_sec: float\n",
    "\n",
    "@dataclass\n",
    "class IngestionResult:\n",
    "    results: List[BatchResult]\n",
    "    failed_rows: List[Dict]\n",
    "\n",
    "# TODO: remove and just use divide_and_conquor_ingest\n",
    "def ingest_batch(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, batch, ingest_mode: str) -> bool:\n",
    "    \"\"\"\n",
    "    Attempts to ingest a batch of data into the specified table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batch: The batch of data to be ingested.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the batch is successfully ingested, False otherwise.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            nrows = cur.adbc_ingest(table_name, [batch], mode=ingest_mode)\n",
    "            print(f'{nrows} rows --> {table_name}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to ingest batch: {e}\")\n",
    "            return False\n",
    "\n",
    "    conn.commit()\n",
    "    return True\n",
    "\n",
    "def divide_and_conquer_ingest(conn: adbc_driver_postgresql.dbapi.Connection, \n",
    "                              table_name: str, \n",
    "                              batch: List[Any], \n",
    "                              ingest_mode: str, \n",
    "                              verbose: bool = True) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Efficient divide and conquer strategy to minimize the number of ingest_batch calls by trying\n",
    "    to ingest larger chunks of the batch and dividing it only upon failure.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batch: The batch of data to be processed.\n",
    "    - verbose (bool): Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of problematic rows that failed to ingest.\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        return []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attempting to ingest batch into {table_name} with {len(batch)} rows.\")\n",
    "\n",
    "    if len(batch) == 1:\n",
    "        # If batch size is 1, attempt to ingest it. If it fails, it's a problematic row.\n",
    "        if not ingest_batch(conn, table_name, batch, ingest_mode):\n",
    "            return batch\n",
    "        return []\n",
    "\n",
    "    if ingest_batch(conn, table_name, batch, ingest_mode):\n",
    "        # Entire batch is successfully ingested, take the win and return an empty list.\n",
    "        return []\n",
    "\n",
    "    # this batch has failed!\n",
    "    mid = len(batch) // 2\n",
    "    left_half = batch[:mid]\n",
    "    right_half = batch[mid:]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Batch failed to ingest, dividing into {len(left_half)} and {len(right_half)} rows.\")\n",
    "\n",
    "    # attempt each half\n",
    "    left_failures = divide_and_conquer_ingest(conn, table_name, left_half, verbose)\n",
    "    right_failures = divide_and_conquer_ingest(conn, table_name, right_half, verbose)\n",
    "\n",
    "    return left_failures + right_failures\n",
    "\n",
    "def handle_batches(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, batches, ingest_mode: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Processes a list of batches, attempting to ingest each one into the specified table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batches: A list of batches to be processed.\n",
    "    - verbose (bool): If True, prints detailed information about the ingestion process. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing an IngestionResult object and a pandas DataFrame summarizing the ingestion process.\n",
    "    \"\"\"\n",
    "\n",
    "    all_failing_rows = []\n",
    "    results = []\n",
    "\n",
    "    for batch in tqdm(batches, desc=\"Ingesting batches\", disable=not verbose):\n",
    "\n",
    "        failing_rows = divide_and_conquer_ingest(conn=conn, table_name=table_name, batch=batch, ingest_mode=ingest_mode)\n",
    "        if(len(failing_rows) > 0):\n",
    "            all_failing_rows.extend(failing_rows)\n",
    "\n",
    "        results.append(BatchResult(table_name, \"hostname_db\", ingest_mode, len(batch), 666666, 666)) # TODO: last 2 fields are [rows, rows/sec]\n",
    "\n",
    "    # Log all failing rows to a JSON lines file (compressed as gz)\n",
    "    with gzip.open(f\"{table_name}_failing_rows.jsonl.gz\", \"wt\") as f:\n",
    "        for row in all_failing_rows:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    df_results = pd.DataFrame([result.__dict__ for result in results])\n",
    "    ir = IngestionResult(results, all_failing_rows)\n",
    "    return ir, df_results\n",
    "\n",
    "#\n",
    "##\n",
    "### pqfile_to_db() - Parquest --> Postgres\n",
    "##\n",
    "#\n",
    "def pqfile_to_db(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, pqfile: str, \n",
    "                   columns_to_jsonb: Optional[List[str]] = None, ingest_mode: str = \"create_append\", \n",
    "                   table_prefix: Optional[str] = \"dev\", verbose: bool = True, show_progress: bool = True, \n",
    "                   batch_size: int = 10000, replace: bool = False) -> List[BatchResult]:\n",
    "    \"\"\"\n",
    "    Ingests a Parquet file into a PostgreSQL database table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: A connection to the PostgreSQL database.\n",
    "    - table_name: The name of the table to write.\n",
    "    - pqfile: The path to the Parquet file.\n",
    "    - columns_to_jsonb: A list of columns to convert to JSONB.\n",
    "    - ingest_mode: The ingest mode (\"create\" or \"append\").\n",
    "    - table_prefix: An optional prefix for the table name.\n",
    "    - verbose: Whether to print verbose output.\n",
    "    - show_progress: Whether to show a progress bar.\n",
    "    - batch_size: The number of rows per batch.\n",
    "    - replace: Whether to replace the table if it exists.\n",
    "\n",
    "    Returns:\n",
    "    - A list of BatchResult objects detailing each batch's ingestion process.\n",
    "    \"\"\"\n",
    "    if columns_to_jsonb is None:\n",
    "        columns_to_jsonb = []\n",
    "\n",
    "    if table_prefix is not None:\n",
    "        table_name = f\"{table_prefix}_{table_name}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{datetime.now()} Ingesting {table_name} to {conn}, ingest_mode={ingest_mode}\")\n",
    "\n",
    "    # read the parquet file into a pyarrow Table object\n",
    "    table = pq.read_table(pqfile)\n",
    "\n",
    "    # TODO: convert Struct and List types to flat JSON strings for import --> PG via ADBC\n",
    "    # for now simply drop them\n",
    "    col_names = set(table.column_names)\n",
    "    cols_to_drop = col_names.intersection(columns_to_jsonb)\n",
    "    table = table.drop_columns(cols_to_drop)\n",
    "\n",
    "    #table = convert_to_jsonb(table, columns_to_jsonb)\n",
    "\n",
    "    # create a list of batches of data to ingest. \n",
    "    batches = table.to_batches(batch_size)\n",
    "\n",
    "    # results_df = pd.DataFrame(columns=['table_name', 'status', 'rows', 'duration', 'rows_per_sec'])\n",
    "\n",
    "    ingestion_result, df_results = handle_batches(conn, table_name, batches=batches, ingest_mode=ingest_mode, verbose=verbose)\n",
    "\n",
    "    display(df_results.head(20))\n",
    "    display(ingestion_result)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ba8b8",
   "metadata": {},
   "source": [
    "## Data Load Begins Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ff9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONN_STR='postgres://username:password@yourhost:5432/gharchive'\n",
    "\n",
    "\n",
    "#\n",
    "# These Columns require flattening to JSON strings (not objects) because ADBC doesn't yet support comnplex types\n",
    "\n",
    "###\n",
    "# ForkEvent                      : payload.forkee.topics: list<item: null>\n",
    "# GollumEvent                    : payload.pages: list<item: struct<action: string, html_url: string, page_name: string, sha: string, title: string>>\n",
    "# IssueCommentEvent              : payload.issue.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# IssueCommentEvent              : payload.issue.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# IssueCommentEvent              : payload.comment.performed_via_github_app.events: list<item: string>\n",
    "# IssuesEvent                    : payload.issue.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# IssuesEvent                    : payload.issue.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestEvent               : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestEvent               : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.requested_teams: list<item: struct<description: string, html_url: string, id: int64, members_url: string, name: string, node_id: string, notification_setting: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestEvent               : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.requested_teams: list<item: struct<description: string, id: int64, members_url: string, name: string, node_id: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string, html_url: string, notification_setting: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent         : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.labels: list<item: struct<color: string, default: bool, description: string, id: int64, name: string, node_id: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.requested_teams: list<item: struct<description: string, html_url: string, id: int64, members_url: string, name: string, node_id: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string, notification_setting: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent         : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PushEvent                      : payload.commits: list<item: struct<author: struct<email: string, name: string>, distinct: bool, message: string, sha: string, url: string>>\n",
    "# ReleaseEvent                   : payload.release.assets: list<item: struct<browser_download_url: string, content_type: string, created_at: string, download_count: int64, id: int64, label: string, name: string, node_id: string, size: int64, state: string, updated_at: string, uploader: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>, url: string>>\n",
    "# ReleaseEvent                   : payload.release.mentions: list<item: struct<avatar_url: string, avatar_user_actor: bool, login: string, profile_url: string, profile_name: string>>\n",
    "###\n",
    "\n",
    "###\n",
    "# https://arrow.apache.org/adbc/0.3.0/driver/cpp/postgresql.html#supported-features\n",
    "#\n",
    "# - Bulk ingestion is supported. The mapping from Arrow types to PostgreSQL types is the same as below.\n",
    "# - Partitioned result sets are not supported.\n",
    "# - The driver makes use of COPY and the binary format to speed up result set reading. Formal benchmarking is forthcoming.\n",
    "# - Transactions are supported.\n",
    "#\n",
    "# PostgreSQL allows defining new types at runtime, so the driver must build a mapping of available types. \n",
    "# This is currently done once at startup. \n",
    "# Type support is currently limited. \n",
    "# \n",
    "# Parameter binding and bulk ingestion support: int16, int32, int64, and string. \n",
    "# Reading result sets is limited to:            int32, int64, float, double, and string.\n",
    "###\n",
    "\n",
    "columns_to_jsonb={\n",
    "        'payload.commits', \n",
    "        'payload.pages', \n",
    "        'payload.release.mentions', \n",
    "        'payload.release.assets',\n",
    "        'payload.pull_request.labels',\n",
    "        'payload.pull_request.requested_reviewers',\n",
    "        'payload.pull_request.requested_teams',\n",
    "        'payload.pull_request.assignees',\n",
    "        'payload.issue.assignees',\n",
    "        'payload.issue.labels',\n",
    "        'payload.issue.body',\n",
    "        'payload.pages',\n",
    "        'payload.forkee.topics',\n",
    "        'payload.pull_request.base.repo.topics',\n",
    "        'payload.comment.performed_via_github_app.events',\n",
    "        'payload.pull_request.base.repo.topics',\n",
    "        'payload.pull_request.head.repo.topics'\n",
    "        }\n",
    "\n",
    "for name, dspath in DATASET_PATHS.items():\n",
    "    print(f\"Processing dataset: {name} : {dspath}\")\n",
    "\n",
    "    with adbc_driver_postgresql.dbapi.connect(CONN_STR) as adbc_conn:\n",
    "        display(adbc_conn)\n",
    "\n",
    "        try:\n",
    "            # use Apache Arrow Database Connector (ADBC) to read Parquet files --> postgres table\n",
    "            num_rows = pqfile_to_db(adbc_conn, name, dspath, table_prefix=f'mattdbg', columns_to_jsonb=columns_to_jsonb)\n",
    "\n",
    "        # TODO: use a more specific exception type\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] Failed to ingest {name} @ {dspath} : {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[INGEST] num_rows={num_rows} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca4aee",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b950fc",
   "metadata": {},
   "source": [
    "## (WIP) Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56511388",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4773b09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summary_df = df.groupby(['partition_key1', 'partition_key2']).agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23591c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def generate_sunburst_chart(summary_df, filename=\"sunburst_chart.png\"):\n",
    "    fig = px.sunburst(summary_df, \n",
    "                      path=['partition_key1', 'partition_key2'], \n",
    "                      values='count', \n",
    "                      color='count',\n",
    "                      title='Data Distribution Across Partitions',\n",
    "                      color_continuous_scale='RdBu')\n",
    "    fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "    #save_plotly_chart_as_png(fig, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662277bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def generate_timeline_chart(summary_df, filename=\"timeline_chart.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_line(point=True).encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Data Counts Over Time'\n",
    "    )\n",
    "    # save_altair_chart_as_png(chart, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5fc79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def generate_scatterplot_over_time(summary_df, filename=\"scatterplot_over_time.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_point().encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        title='Scatterplot of Data Over Time',\n",
    "        width=800,\n",
    "        height=400\n",
    "    ).interactive()  # Enables panning and zooming\n",
    "    # save_altair_chart_as_png(chart, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

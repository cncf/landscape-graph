{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dataset: Swift and First Party Libraries (apple/swift-*, swift-server/*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Size and Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from notebook_utils import *\n",
    "# from arrow_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"---\"\n",
    "!ls     ~/gharchive-swift\n",
    "!echo \"---\"\n",
    "!du -hs ~/gharchive-swift\n",
    "!echo \"---\"\n",
    "!du -hs ~/gharchive-swift/*gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!echo \"---\"\n",
    "!ls     ~/gharchive-swift/swift.all\n",
    "!echo \"---\"\n",
    "!du -hs ~/gharchive-swift/swift.all/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset: Orgs and Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# each dataset has an org/repo directory structure.\n",
    "!tree -L 2 -d /Users/matt/gharchive-swift/swift.all/PullRequestEvent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dataset Time Range: 2015 - 2024 (thru Feb 2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /Users/matt/gharchive-swift/swift.all/PullRequestEvent/apple/swift | head -n 10\n",
    "!echo \"---\"\n",
    "!ls -lh /Users/matt/gharchive-swift/swift.all/PullRequestEvent/apple/swift | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup, Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "# import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "DATASETS_ROOT_PATH=\"/Users/matt/gharchive-swift/swift.all\"\n",
    "DATASETS_ROOT = Path(DATASETS_ROOT_PATH)\n",
    "DATASET_PATHS = {}\n",
    "\n",
    "for subdir in os.listdir(DATASETS_ROOT):\n",
    "    subdir_path = DATASETS_ROOT / subdir\n",
    "    if subdir_path.is_dir():\n",
    "        DATASET_PATHS[subdir] = subdir_path\n",
    "\n",
    "DATASET_PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {}\n",
    "\n",
    "for name, path in DATASET_PATHS.items():\n",
    "    print(f'Processing: {name} -> {path}')\n",
    "    DATASETS[name] = load_parquet_dataset(name, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Persist per-event Schema -> files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in DATASETS.items():\n",
    "    print(f'name: {name}')\n",
    "    with open(f'{name}.schema', 'a') as f:\n",
    "        f.write(str(dataset.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l *.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WatchEvent.Schema (example 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat WatchEvent.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReleaseEvent Schema (example 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ReleaseEvent.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_summaries = {}\n",
    "\n",
    "def dataset_schema_summary(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a summary DataFrame for all fragments in a dataset, including schema details\n",
    "    and partition information.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: The file system path to the dataset.\n",
    "\n",
    "    Returns:\n",
    "    A pandas DataFrame with columns for each schema field, including fragment and partition keys.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(dataset_path, format=\"parquet\")  # Adjust format as needed\n",
    "    summary = []\n",
    "\n",
    "    for fragment in dataset.get_fragments():\n",
    "        schema = fragment.physical_schema\n",
    "        \n",
    "        for field in schema:\n",
    "            summary.append({\n",
    "                \"Fragment\": fragment.path,\n",
    "                \"Field Name\": field.name,\n",
    "                \"Type\": str(field.type),\n",
    "                \"Nullable\": field.nullable\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the summary list\n",
    "    df = pd.DataFrame(summary)\n",
    "    print(f'{df.shape}')\n",
    "    df.drop_duplicates(inplace=True, subset=[\"Field Name\", \"Type\", \"Nullable\"])\n",
    "    print(f'{df.shape}')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Zone (WIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: after we create monthlies, too expensive to run now\n",
    "# for name, path in DATASET_PATHS.items():\n",
    "#     print(f'name: {name}')\n",
    "#     df = dataset_schema_summary(path)\n",
    "#     schema_summaries[name] = df\n",
    "# schema_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Assuming `df` is your DataFrame loaded from the Parquet dataset\n",
    "summary_df = df.groupby(['partition_key1', 'partition_key2']).agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def generate_sunburst_chart(summary_df, filename=\"sunburst_chart.png\"):\n",
    "    fig = px.sunburst(summary_df, \n",
    "                      path=['partition_key1', 'partition_key2'],  # Adjust based on your dataset\n",
    "                      values='count',  # This should be your aggregate column\n",
    "                      color='count',\n",
    "                      title='Data Distribution Across Partitions',\n",
    "                      color_continuous_scale='RdBu')\n",
    "    fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "    save_plotly_chart_as_png(fig, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def generate_timeline_chart(summary_df, filename=\"timeline_chart.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_line(point=True).encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Data Counts Over Time'\n",
    "    )\n",
    "    save_altair_chart_as_png(chart, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def generate_scatterplot_over_time(summary_df, filename=\"scatterplot_over_time.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_point().encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        title='Scatterplot of Data Over Time',\n",
    "        width=800,\n",
    "        height=400\n",
    "    ).interactive()  # Enables panning and zooming\n",
    "    save_altair_chart_as_png(chart, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

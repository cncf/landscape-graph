{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8976e1b4",
   "metadata": {},
   "source": [
    "# CNCF and Apache Ecosystem (culled from gharchive.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e79e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from time import time\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "import gzip\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1024)\n",
    "pd.set_option('display.max_columns', 512)\n",
    "pd.set_option('display.width', 1024)\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import adbc_driver_postgresql.dbapi\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import panel as pn\n",
    "import altair as alt    # https://altair-viz.github.io/\n",
    "import vegafusion as vf # https://vegafusion.io/\n",
    "\n",
    "import simdjson\n",
    "\n",
    "from notebook_utils import *\n",
    "#from arrow_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0918cc8",
   "metadata": {},
   "source": [
    "## Dataset: Size and Scope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd074a8",
   "metadata": {},
   "source": [
    "### JSON Dataset (per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71112498",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "DATASETS_ROOT_PATH=Path(\"~/gharchive-cncf/cncf.byrepo.consolidated.json\").expanduser()\n",
    "DATASETS_ROOT = Path(DATASETS_ROOT_PATH)\n",
    "DATASET_PATHS = {}\n",
    "\n",
    "for file in DATASETS_ROOT.glob(\"*.parquet\"):\n",
    "    DATASET_PATHS[file.stem] = file\n",
    "display(DATASET_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196c68d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_parquet_dataset(name: str, path: str) -> pq.ParquetDataset:\n",
    "    #print(f'Loading dataset: {name} from {path}')\n",
    "    pqds = pq.ParquetDataset(path, memory_map=True)\n",
    "    return pqds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6de377",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {}\n",
    "DATASET_SCHEMAS = {}\n",
    "\n",
    "for name, path in DATASET_PATHS.items():\n",
    "    DATASETS[name] = load_parquet_dataset(name, path)\n",
    "\n",
    "for name, dataset in DATASETS.items():\n",
    "    DATASET_SCHEMAS[name] = dataset.schema\n",
    "\n",
    "display(DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7d7e55",
   "metadata": {},
   "source": [
    "### Persist Event Schema to files (.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e560f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "schema_summaries = {}\n",
    "\n",
    "def dataset_schema_summary(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a summary DataFrame for all fragments in a dataset, including schema details\n",
    "    and partition information.\n",
    "\n",
    "    Parameters:\n",
    "    - dataset_path: The file system path to the dataset.\n",
    "\n",
    "    Returns:\n",
    "    A pandas DataFrame with columns for each schema field, including fragment and partition keys.\n",
    "    \"\"\"\n",
    "    dataset = ds.dataset(dataset_path, format=\"parquet\")  # Adjust format as needed\n",
    "    summary = []\n",
    "\n",
    "    for fragment in dataset.get_fragments():\n",
    "        schema = fragment.physical_schema\n",
    "        \n",
    "        for field in schema:\n",
    "            summary.append({\n",
    "                \"Fragment\": fragment.path,\n",
    "                \"Field Name\": field.name,\n",
    "                \"Type\": str(field.type),\n",
    "                \"Nullable\": field.nullable\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the summary list\n",
    "    df = pd.DataFrame(summary)\n",
    "    print(f'{df.shape}')\n",
    "    df.drop_duplicates(inplace=True, subset=[\"Field Name\", \"Type\", \"Nullable\"])\n",
    "    print(f'{df.shape}')\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_table(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, table: pa.Table):\n",
    "    \"\"\"\n",
    "    Replace the contents of a specified table with new data from an Arrow Table, preserving the table's schema and indexes.\n",
    "\n",
    "    This function creates a temporary table with the new data, renames the existing table to a temporary name,\n",
    "    renames the new table to the original table name, and finally drops the old table. This approach\n",
    "    avoids the need to recreate indexes, which can be expensive for large tables.\n",
    "\n",
    "    Parameters:\n",
    "    - conn (adbc_driver_postgresql.dbapi.Connection): A connection to the PostgreSQL database.\n",
    "    - table_name (str): The name of the table to be replaced.\n",
    "    - table (pa.Table): An Arrow Table containing the new data to replace the existing table's contents.\n",
    "\n",
    "    Note:\n",
    "    - The function uses the ADBC framework's automatic transaction management. If an error occurs during\n",
    "      the process, the transaction is automatically rolled back to maintain data integrity.\n",
    "    - The renaming of the table does not change the table's schema or indexes. The indexes will retain their\n",
    "      original names and continue to reference the renamed table.\n",
    "    - This function assumes that the `adbc_ingest` method is available for creating the temporary table\n",
    "      with the desired schema and data.\n",
    "    \"\"\"\n",
    "    temp_table_name = f\"{table_name}_temp\"\n",
    "    old_table_name = f\"{table_name}_old\"\n",
    "    \n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.adbc_ingest(temp_table_name, table, mode=\"create\", temporary=True)\n",
    "            cur.execute(f\"ALTER TABLE {table_name} RENAME TO {old_table_name}\")\n",
    "            cur.execute(f\"ALTER TABLE {temp_table_name} RENAME TO {table_name}\")\n",
    "            cur.execute(f\"DROP TABLE {old_table_name}\")\n",
    "    except adbc_driver_postgresql.dbapi.Error as e:\n",
    "        # The transaction is automatically rolled back by the ADBC framework if an error occurs\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c03a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_schema_prefix(name: str) -> str:\n",
    "    \"\"\"\n",
    "    if prefix found, remove it from the name used for schema file\n",
    "\n",
    "    names cam be {event_type}.schema, or '{prefix}-{event_type}.schema'\n",
    "    \"\"\"\n",
    "    parts = name.split('-', 1)\n",
    "    if len(parts) > 1:\n",
    "        return parts[1]\n",
    "    else:\n",
    "        return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8946256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_markdown_documentation(schema: pa.Schema, title: str = \"Schema Documentation\") -> str:\n",
    "    \"\"\"\n",
    "    Generates Markdown documentation for a PyArrow Schema object, formatted for GitHub with improved table formatting.\n",
    "\n",
    "    Parameters:\n",
    "    - schema: The PyArrow Schema object to document.\n",
    "    - title: The title of the documentation. Default is \"Schema Documentation\".\n",
    "\n",
    "    Returns:\n",
    "    - A string containing the Markdown documentation.\n",
    "    \"\"\"\n",
    "    markdown = f\"# {title}\\n\\n\"\n",
    "    markdown += \"## Table of Contents\\n\\n\"\n",
    "    toc = []\n",
    "\n",
    "    # Function to calculate padding for each column\n",
    "    def calculate_padding(fields):\n",
    "        max_name_length = max((len(field.name) for field in fields), default=0)\n",
    "        max_type_length = max((len(str(field.type)) for field in fields), default=0)\n",
    "        return max_name_length, max_type_length\n",
    "\n",
    "    # Calculate padding for the top-level fields\n",
    "    max_name_length, max_type_length = calculate_padding(schema)\n",
    "\n",
    "    # Start the table\n",
    "    markdown += f\"| {'Field Name'.ljust(max_name_length)} | {'Type'.ljust(max_type_length)} |\\n\"\n",
    "    markdown += f\"| {'-'*max_name_length} | {'-'*max_type_length} |\\n\"\n",
    "\n",
    "    for field in schema:\n",
    "        field_name = field.name.ljust(max_name_length)\n",
    "        field_type = str(field.type).ljust(max_type_length)\n",
    "        markdown += f\"| {field_name} | {field_type} |\\n\"\n",
    "        if isinstance(field.type, pa.StructType):\n",
    "            # Calculate padding for nested fields\n",
    "            max_nested_name_length, max_nested_type_length = calculate_padding(field.type)\n",
    "            for sub_field in field.type:\n",
    "                nested_name = sub_field.name.ljust(max_nested_name_length)\n",
    "                nested_type = str(sub_field.type).ljust(max_nested_type_length)\n",
    "                markdown += f\"| {' '*max_name_length} | {nested_name} | {nested_type} |\\n\"\n",
    "        toc.append(f\"- [{field.name}](#{field.name.lower().replace(' ', '-')})\\n\")\n",
    "\n",
    "    markdown += \"\\n## Table of Contents\\n\\n\" + \"\".join(toc) + \"\\n\\n\"\n",
    "    return markdown\n",
    "\n",
    "# note: pyarrow.schema() is a factory function that returns a pyarrow.Schema object\n",
    "schema = pa.schema([\n",
    "    ('some_int', pa.int32()),\n",
    "    ('some_string', pa.string()),\n",
    "    ('some_struct', pa.struct([\n",
    "        ('f1', pa.int32()),\n",
    "        ('f2', pa.string())\n",
    "    ]))\n",
    "])\n",
    "\n",
    "print(generate_markdown_documentation(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfdd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_markdown_docs(schemas: Dict[str, pa.Schema], output_dir: str):\n",
    "    \"\"\"\n",
    "    Generates Markdown documentation for each schema in the provided dictionary and saves them to files.\n",
    "\n",
    "    Parameters:\n",
    "    - schemas: A dictionary mapping event types to PyArrow schema objects.\n",
    "    - output_dir: The directory where the Markdown files will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Iterate over the schemas and generate Markdown documentation\n",
    "    for event_type, schema in schemas.items():\n",
    "        # Generate the Markdown documentation\n",
    "        markdown_doc = generate_markdown_documentation(schema, title=event_type)\n",
    "\n",
    "        # Save the Markdown documentation to a file\n",
    "        filename = f\"{strip_schema_prefix(event_type)}.md\"\n",
    "        with open(os.path.join(output_dir, filename), 'w') as f:\n",
    "            f.write(markdown_doc)\n",
    "\n",
    "schemas = {\n",
    "    'IssuesEvent': pa.schema([\n",
    "        ('actor', pa.struct([\n",
    "            ('avatar_url', pa.string()),\n",
    "            ('display_login', pa.string()),\n",
    "            ('gravatar_id', pa.string()),\n",
    "            ('id', pa.int64()),\n",
    "            ('login', pa.string()),\n",
    "            ('url', pa.string())\n",
    "        ])),\n",
    "        # ... other fields ...\n",
    "    ]),\n",
    "    # ... other event types ...\n",
    "}\n",
    "\n",
    "# Call the function to generate and save Markdown documentation\n",
    "# create_and_save_markdown_docs(schemas, output_dir='./test-docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4d01e5",
   "metadata": {},
   "source": [
    "### Create .schema files and documentation\n",
    "\n",
    "Regen the schema files from the parquet dataset.  since they are commited data/gharchive/schema/*.schema, it's easy to see any deviation and/or changes to the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77681d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proj_relative_root = '../..'\n",
    "proj_relative_root = '.'\n",
    "\n",
    "for name, dataset in DATASETS.items():\n",
    "    schema_file = f'{proj_relative_root}/data/gharchive/schema/{strip_schema_prefix(name)}.schema'\n",
    "\n",
    "    with open(schema_file, 'w') as f:\n",
    "        nbytes = f.write(str(dataset.schema))\n",
    "\n",
    "    print(f'dataset: {name} -> {schema_file}, {nbytes} bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc7ab",
   "metadata": {},
   "source": [
    "### Generate Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1fcb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATASET_SCHEMAS.keys())\n",
    "\n",
    "doc_dir = f'{proj_relative_root}/data/gharchive/docs'\n",
    "create_and_save_markdown_docs(DATASET_SCHEMAS, doc_dir)\n",
    "\n",
    "!ls -l $doc_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4e9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BatchResult:\n",
    "    table_name: str\n",
    "    hostname_db: str\n",
    "    ingest_mode: str\n",
    "    rows: int\n",
    "    duration: float\n",
    "    rows_per_sec: float\n",
    "\n",
    "@dataclass\n",
    "class IngestionResult:\n",
    "    results: List[BatchResult]\n",
    "    failed_rows: List[Dict]\n",
    "\n",
    "def ingest_batch(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, batch, ingest_mode: str) -> bool:\n",
    "    \"\"\"\n",
    "    Attempts to ingest a batch of data into the specified table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batch: The batch of data to be ingested.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if the batch is successfully ingested, False otherwise.\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cur:\n",
    "        try:\n",
    "            nrows = cur.adbc_ingest(table_name, [batch], mode=ingest_mode)\n",
    "            print(f'{nrows} rows --> {table_name}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to ingest batch: {e}\")\n",
    "            return False\n",
    "\n",
    "    conn.commit()\n",
    "    return True\n",
    "\n",
    "def divide_and_conquer_ingest(conn: adbc_driver_postgresql.dbapi.Connection, \n",
    "                              table_name: str, batch: List[Any], ingest_mode: str, verbose: bool = True) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Efficient divide and conquer strategy to minimize the number of ingest_batch calls by trying\n",
    "    to ingest larger chunks of the batch and dividing it only upon failure.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batch: The batch of data to be processed.\n",
    "    - verbose (bool): If True, prints detailed information about the process. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of problematic rows that failed to ingest.\n",
    "    \"\"\"\n",
    "    if not batch:\n",
    "        return []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Attempting to ingest batch into {table_name} with {len(batch)} rows.\")\n",
    "\n",
    "    if len(batch) == 1:\n",
    "        # If batch size is 1, attempt to ingest it. If it fails, it's a problematic row.\n",
    "        if not ingest_batch(conn, table_name, batch, ingest_mode):\n",
    "            return batch\n",
    "        return []\n",
    "\n",
    "    if ingest_batch(conn, table_name, batch, ingest_mode):\n",
    "        # If the entire batch is successfully ingested, return an empty list.\n",
    "        return []\n",
    "\n",
    "    # If the batch ingestion fails, divide the batch into two and try again.\n",
    "    mid = len(batch) // 2\n",
    "    left_half = batch[:mid]\n",
    "    right_half = batch[mid:]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Batch failed to ingest, dividing into {len(left_half)} and {len(right_half)} rows.\")\n",
    "\n",
    "    left_failures = divide_and_conquer_ingest(conn, table_name, left_half, verbose)\n",
    "    right_failures = divide_and_conquer_ingest(conn, table_name, right_half, verbose)\n",
    "\n",
    "    return left_failures + right_failures\n",
    "\n",
    "\n",
    "def handle_batches(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, batches, ingest_mode: str, verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Processes a list of batches, attempting to ingest each one into the specified table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: An ADBC database connection object.\n",
    "    - table_name (str): The name of the table to ingest data into.\n",
    "    - batches: A list of batches to be processed.\n",
    "    - verbose (bool): If True, prints detailed information about the ingestion process. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: A tuple containing an IngestionResult object and a pandas DataFrame summarizing the ingestion process.\n",
    "    \"\"\"\n",
    "\n",
    "    all_failing_rows = []\n",
    "    results = []\n",
    "\n",
    "    for batch in tqdm(batches, desc=\"Ingesting batches\", disable=not verbose):\n",
    "\n",
    "        # TODO: Call divide_and_conquer_ingest() to handle the batch entirely once debugged.\n",
    "        if not ingest_batch(conn, table_name, batch, ingest_mode):\n",
    "            \n",
    "            # If ingestion fails, use divide and conquer to find the problematic rows\n",
    "            failing_rows = divide_and_conquer_ingest(conn, table_name, batch, ingest_mode=ingest_mode)\n",
    "            all_failing_rows.extend(failing_rows)\n",
    "\n",
    "        else:\n",
    "            # If ingestion is successful, no need for divide and conquer\n",
    "            failing_rows = []\n",
    "\n",
    "        results.append(BatchResult(table_name, \"hostname_db\", ingest_mode, len(batch), 666, 666))\n",
    "\n",
    "    # Log all failing rows to a JSON lines file (compressed as gz)\n",
    "    with gzip.open(f\"{table_name}_failing_rows.jsonl.gz\", \"wt\") as f:\n",
    "        for row in all_failing_rows:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    #  result.__dict__ for each result in results converts each BatchResult instance into a dictionary. \n",
    "    # This is done to easily convert the list of BatchResult instances into a format that can be used \n",
    "    # to create a pandas DataFrame. Each dictionary represents the attributes of a BatchResult instance,\n",
    "    # which are then used to construct the DataFrame.\n",
    "    #\n",
    "    # This approach is efficient and straightforward for creating a summary of the ingestion process, as \n",
    "    # it leverages the built-in __dict__ attribute to dynamically access the attributes of each BatchResult instance\n",
    "    \n",
    "    summary_df = pd.DataFrame([result.__dict__ for result in results])\n",
    "    return IngestionResult(results, all_failing_rows), summary_df\n",
    "\n",
    "#\n",
    "##\n",
    "### pqfile_to_db() - Parquest --> Postgres\n",
    "##\n",
    "#\n",
    "def pqfile_to_db(conn: adbc_driver_postgresql.dbapi.Connection, table_name: str, pqfile: str, \n",
    "                   columns_to_jsonb: Optional[List[str]] = None, ingest_mode: str = \"create_append\", \n",
    "                   table_prefix: Optional[str] = \"dev\", verbose: bool = True, show_progress: bool = True, \n",
    "                   batch_size: int = 10000, replace: bool = False) -> List[BatchResult]:\n",
    "    \"\"\"\n",
    "    Ingests a Parquet file into a PostgreSQL database table.\n",
    "\n",
    "    Parameters:\n",
    "    - conn: A connection to the PostgreSQL database.\n",
    "    - table_name: The name of the table to write.\n",
    "    - pqfile: The path to the Parquet file.\n",
    "    - columns_to_jsonb: A list of columns to convert to JSONB.\n",
    "    - ingest_mode: The ingest mode (\"create\" or \"append\").\n",
    "    - table_prefix: An optional prefix for the table name.\n",
    "    - verbose: Whether to print verbose output.\n",
    "    - show_progress: Whether to show a progress bar.\n",
    "    - batch_size: The number of rows per batch.\n",
    "    - replace: Whether to replace the table if it exists.\n",
    "\n",
    "    Returns:\n",
    "    - A list of BatchResult objects detailing each batch's ingestion process.\n",
    "    \"\"\"\n",
    "    if columns_to_jsonb is None:\n",
    "        columns_to_jsonb = []\n",
    "\n",
    "    if table_prefix is not None:\n",
    "        table_name = f\"{table_prefix}_{table_name}\"\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"{datetime.now()} Ingesting {table_name} to {conn}, ingest_mode={ingest_mode}\")\n",
    "\n",
    "    table = pq.read_table(pqfile)\n",
    "\n",
    "    # TODO: when we get the rest imported.  for now skipping these\n",
    "    col_names = set(table.column_names)\n",
    "    cols_to_drop = col_names.intersection(columns_to_jsonb)\n",
    "    table = table.drop_columns(cols_to_drop)\n",
    "\n",
    "    #table = convert_to_jsonb(table, columns_to_jsonb)\n",
    "\n",
    "    batches = table.to_batches(batch_size)\n",
    "\n",
    "    # results_df = pd.DataFrame(columns=['table_name', 'status', 'rows', 'duration', 'rows_per_sec'])\n",
    "\n",
    "    ingestion_result, df_result = handle_batches(conn, table_name, batches=batches, ingest_mode=ingest_mode, verbose=verbose)\n",
    "\n",
    "    display(df_result.head())\n",
    "\n",
    "    \n",
    "    return \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "#         with conn.cursor() as cur:\n",
    "#             for batch in tqdm(batches, desc=\"Ingesting batches\", disable=not show_progress):\n",
    "#                 stopwatch = Stopwatch()\n",
    "#                 stopwatch.start()\n",
    "#                 try:\n",
    "#                     # start moving actual data on the wire\n",
    "#                     nrows = cur.adbc_ingest(table_name, [batch], mode=ingest_mode)\n",
    "\n",
    "#                     duration = stopwatch.stop()\n",
    "#                     rows_per_sec = nrows / duration if duration else 0\n",
    "\n",
    "#                     results_df = results_df.append({'table_name': table_name, 'status': 'success', 'rows': nrows, 'duration': duration, 'rows_per_sec': rows_per_sec}, ignore_index=True)\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Failed to ingest batch: {e}\")\n",
    "                    \n",
    "#                     _, failing_rows = binary_search_for_failure(conn, table_name, batch, verbose=verbose)\n",
    "\n",
    "#                     for row in failing_rows:\n",
    "#                         results_df = results_df.append({'table_name': table_name, 'status': 'fail', 'rows': 1, 'duration': 0, 'rows_per_sec': 0}, ignore_index=True)\n",
    "#                     continue # Continue with the next batch\n",
    "\n",
    "#         conn.commit()\n",
    "\n",
    "#         if verbose:\n",
    "#             print(f\"{datetime.now()} Finished ingesting {table_name}\")\n",
    "\n",
    "#         return results_df  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ba8b8",
   "metadata": {},
   "source": [
    "## Data Load Begins Here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ff9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONN_STR='postgres://matt:niftypwd@osrb-approved-ospo-tools-6din.postgres.db.aci.apple.com:5432/osrb'\n",
    "\n",
    "# ForkEvent.schema:118:payload.forkee.topics: list<item: null>\n",
    "# PullRequestEvent.schema:395:payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestEvent.schema:401:payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent.schema:473:payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent.schema:477:payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent.schema:399:payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent.schema:403:payload.pull_request.head.repo.topics: list<item: string>\n",
    "\n",
    "\n",
    "# ForkEvent                      : payload.forkee.topics: list<item: null>\n",
    "# GollumEvent                    : payload.pages: list<item: struct<action: string, html_url: string, page_name: string, sha: string, title: string>>\n",
    "# IssueCommentEvent              : payload.issue.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# IssueCommentEvent              : payload.issue.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# IssueCommentEvent              : payload.comment.performed_via_github_app.events: list<item: string>\n",
    "# IssuesEvent                    : payload.issue.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# IssuesEvent                    : payload.issue.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestEvent               : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestEvent               : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.requested_teams: list<item: struct<description: string, html_url: string, id: int64, members_url: string, name: string, node_id: string, notification_setting: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string>>\n",
    "# PullRequestEvent               : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestEvent               : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.labels: list<item: struct<color: string, default: bool, id: int64, name: string, node_id: string, url: string, description: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.requested_teams: list<item: struct<description: string, id: int64, members_url: string, name: string, node_id: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string, html_url: string, notification_setting: string>>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewCommentEvent  : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent         : payload.pull_request.assignees: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.labels: list<item: struct<color: string, default: bool, description: string, id: int64, name: string, node_id: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.requested_reviewers: list<item: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.requested_teams: list<item: struct<description: string, html_url: string, id: int64, members_url: string, name: string, node_id: string, permission: string, privacy: string, repositories_url: string, slug: string, url: string, notification_setting: string>>\n",
    "# PullRequestReviewEvent         : payload.pull_request.base.repo.topics: list<item: string>\n",
    "# PullRequestReviewEvent         : payload.pull_request.head.repo.topics: list<item: string>\n",
    "# PushEvent                      : payload.commits: list<item: struct<author: struct<email: string, name: string>, distinct: bool, message: string, sha: string, url: string>>\n",
    "# ReleaseEvent                   : payload.release.assets: list<item: struct<browser_download_url: string, content_type: string, created_at: string, download_count: int64, id: int64, label: string, name: string, node_id: string, size: int64, state: string, updated_at: string, uploader: struct<avatar_url: string, events_url: string, followers_url: string, following_url: string, gists_url: string, gravatar_id: string, html_url: string, id: int64, login: string, node_id: string, organizations_url: string, received_events_url: string, repos_url: string, site_admin: bool, starred_url: string, subscriptions_url: string, type: string, url: string>, url: string>>\n",
    "# ReleaseEvent                   : payload.release.mentions: list<item: struct<avatar_url: string, avatar_user_actor: bool, login: string, profile_url: string, profile_name: string>>\n",
    "columns_to_jsonb={\n",
    "        'payload.commits', \n",
    "        'payload.pages', \n",
    "        'payload.release.mentions', \n",
    "        'payload.release.assets',\n",
    "        'payload.pull_request.labels',\n",
    "        'payload.pull_request.requested_reviewers',\n",
    "        'payload.pull_request.requested_teams',\n",
    "        'payload.pull_request.assignees',\n",
    "        'payload.issue.assignees',\n",
    "        'payload.issue.labels',\n",
    "        'payload.issue.body',\n",
    "        'payload.pages',\n",
    "        'payload.forkee.topics',\n",
    "        'payload.pull_request.base.repo.topics',\n",
    "        'payload.comment.performed_via_github_app.events',\n",
    "        'payload.pull_request.base.repo.topics',\n",
    "        'payload.pull_request.head.repo.topics'\n",
    "        }\n",
    "\n",
    "for name, dspath in DATASET_PATHS.items():\n",
    "    print(f\"Processing dataset: {name} : {dspath}\")\n",
    "\n",
    "    # https://arrow.apache.org/adbc/0.3.0/driver/cpp/postgresql.html#supported-features\n",
    "\n",
    "    # - Bulk ingestion is supported. The mapping from Arrow types to PostgreSQL types is the same as below.\n",
    "    # - Partitioned result sets are not supported.\n",
    "    # - The driver makes use of COPY and the binary format to speed up result set reading. Formal benchmarking is forthcoming.\n",
    "    # - Transactions are supported.\n",
    "    \n",
    "    # PostgreSQL allows defining new types at runtime, so the driver must build a mapping of available types. This is currently done once at startup.\n",
    "    # Type support is currently limited. \n",
    "    # \n",
    "    # Parameter binding and bulk ingestion support: int16, int32, int64, and string. \n",
    "    # Reading result sets is limited to:            int32, int64, float, double, and string.\n",
    "    with adbc_driver_postgresql.dbapi.connect(CONN_STR) as adbc_conn:\n",
    "        display(adbc_conn)\n",
    "\n",
    "        try:                   num_rows = pqfile_to_db(adbc_conn, name, dspath, table_prefix=f'mattdbg', columns_to_jsonb=columns_to_jsonb)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to ingest {name} : {e}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Inserted {num_rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabb181",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD STUFF\n",
    "\n",
    "\n",
    "\n",
    "# from datetime import datetime\n",
    "\n",
    "# # TODO move to JSONB for most of these...\n",
    "\n",
    "# columns_to_drop={'payload.commits', \n",
    "#                  'payload.pages', \n",
    "#                  'payload.release.mentions', \n",
    "#                  'payload.release.assets',\n",
    "#                  'payload.pull_request.labels',\n",
    "#                  'payload.pull_request.requested_reviewers',\n",
    "#                  'payload.pull_request.requested_teams',\n",
    "#                  'payload.pull_request.assignees',\n",
    "#                  'payload.issue.assignees',\n",
    "#                  'payload.issue.labels',\n",
    "#                  'payload.issue.`body`',\n",
    "#                  'payload.pages',\n",
    "#                  'payload.forkee.topics',\n",
    "#                  'payload.pull_request.base.repo.topics',\n",
    "#                  'payload.comment.performed_via_github_app.events',\n",
    "#                  'payload.pull_request.base.repo.topics',\n",
    "#                  'payload.pull_request.head.repo.topics'}\n",
    "\n",
    "# def pqfile_to_db(conn, table_name=name, pqfile=pq.ParquetFile, ingest_mode=\"create\", table_prefix = \"dev\",  verbose=True, ):\n",
    "#     \"\"\"\n",
    "#     Write a dataset to a database as a table\n",
    "\n",
    "#     Parameters:\n",
    "#     - conn        : A connection to the database.\n",
    "#     - table_name  : The name of the table to write.\n",
    "#     - dataset     : The ParquetDataset Table.\n",
    "#     - ingest_mode : The ingest mode to use.\n",
    "#     - verbose     : Whether to print verbose output.\n",
    "#     \"\"\"\n",
    "#     ic()\n",
    "\n",
    "#     if table_prefix is not None:\n",
    "#         table_name = f\"{table_prefix}_{table_name}\"\n",
    "\n",
    "#     if verbose:\n",
    "#         msg = f\"{datetime.now()} Ingesting {table_name} to {conn}, ingest_mode={ingest_mode}\"\n",
    "#         ic(msg)\n",
    "\n",
    "#     with conn.cursor() as cur:\n",
    "#         # pqfile = pq.ParquetFile(pqfile)\n",
    "#         table = pq.read_table(pqfile)\n",
    "\n",
    "#         col_names = set(table.column_names)\n",
    "#         cols_to_drop = col_names.intersection(columns_to_drop)\n",
    "#         table = table.drop_columns(cols_to_drop)\n",
    "\n",
    "#         nrows = cur.adbc_ingest(table_name, table.to_batches(), mode=\"create\")\n",
    "\n",
    "#         conn.commit()\n",
    "\n",
    "#         if verbose:\n",
    "#             ic(f\"{datetime.now()} Finished ingesting {table_name} \")\n",
    "\n",
    "#         return nrows\n",
    "\n",
    "# CONN_STR='postgres://matt:niftypwd@osrb-approved-ospo-tools-6din.postgres.db.aci.apple.com:5432/osrb'\n",
    "\n",
    "# for name, dspath in DATASET_PATHS.items():\n",
    "#     print(f\"Processing dataset: {name} : {dspath}\")\n",
    "\n",
    "#     with adbc_driver_postgresql.dbapi.connect(CONN_STR) as adbc_conn:\n",
    "#         display(adbc_conn)\n",
    "\n",
    "#         try:        \n",
    "#             num_rows = pqfile_to_db(adbc_conn, name, dspath)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to ingest {name} : {e}\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"Inserted {num_rows} rows\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aca4aee",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b950fc",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4773b09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "summary_df = df.groupby(['partition_key1', 'partition_key2']).agg('count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23591c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def generate_sunburst_chart(summary_df, filename=\"sunburst_chart.png\"):\n",
    "    fig = px.sunburst(summary_df, \n",
    "                      path=['partition_key1', 'partition_key2'], \n",
    "                      values='count', \n",
    "                      color='count',\n",
    "                      title='Data Distribution Across Partitions',\n",
    "                      color_continuous_scale='RdBu')\n",
    "    fig.update_layout(margin=dict(t=0, l=0, r=0, b=0))\n",
    "    #save_plotly_chart_as_png(fig, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662277bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "def generate_timeline_chart(summary_df, filename=\"timeline_chart.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_line(point=True).encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        title='Data Counts Over Time'\n",
    "    )\n",
    "    # save_altair_chart_as_png(chart, filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba5fc79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def generate_scatterplot_over_time(summary_df, filename=\"scatterplot_over_time.png\"):\n",
    "    chart = alt.Chart(summary_df).mark_point().encode(\n",
    "        x='time:T',  # Adjust for your time-related partition key\n",
    "        y='count:Q',\n",
    "        tooltip=['partition_key1', 'partition_key2', 'count']  # Adjust tooltips as needed\n",
    "    ).properties(\n",
    "        title='Scatterplot of Data Over Time',\n",
    "        width=800,\n",
    "        height=400\n",
    "    ).interactive()  # Enables panning and zooming\n",
    "    # save_altair_chart_as_png(chart, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

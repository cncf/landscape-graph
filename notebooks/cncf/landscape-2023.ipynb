{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNCF Landscape: {Category, Subcategory, Project } --> Stars, Commits, Contributors, Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On JupyterLab Shell Integration(s) and SList\n",
    "\n",
    "More Info: _http://safaribooksonline.com/blog/2014/02/12/using-shell-commands-effectively-ipython_\n",
    "\n",
    "---\n",
    "\n",
    "> SList instances can be used like a regular list, but they provide several methods that are useful when working with shell output. The main properties available in an SList instance are:\n",
    "> \n",
    "> * `.s` returns the elements joined together by spaces. \n",
    ">   * _This is useful for building command lines that take many arguments in a single invocation._\n",
    "> * `.n` returns the elements joined together by a newline. \n",
    ">   * _Use this when you need the original output unmodified._\n",
    "> * `.p` returns the elements as path objects, if they are filenames.\n",
    ">   * _Use this when doing more advanced path manipulation_\n",
    "> \n",
    "> In addition, SList instances support `grep()` and `fields()` methods. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ai help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 512)\n",
    "pd.set_option('display.max_columns', 512)\n",
    "pd.set_option('display.width', 512)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all generated output files land here\n",
    "OUT_DIR='generated'\n",
    "\n",
    "# TODO: factor out landscape ('cncf') so this can be used for landscape(s) generically (https://landscapes.dev) \n",
    " \n",
    "CNCF_LANDSCAPE_FNAME_BASE='cncf-landscape'\n",
    "CNCF_LANDSCAPE_FNAME_ROOT=f'{OUT_DIR}/{CNCF_LANDSCAPE_FNAME_BASE}'\n",
    "\n",
    "CNCF_PROJECTS_FNAME_BASE=f'cncf-projects'\n",
    "CNCF_PROJECTS_FNAME_ROOT=f'{OUT_DIR}/{CNCF_PROJECTS_FNAME_BASE}'\n",
    "\n",
    "print(f'Jupyter Kernel (venv): {sys.executable}')\n",
    "print(f'Output Location:       {OUT_DIR}  (.json, .jsonl, .csv, .md, .svg, .png, ...)')\n",
    "print(f'Output Landscape root: {CNCF_LANDSCAPE_FNAME_ROOT}')\n",
    "print(f'Output Projects  root: {CNCF_PROJECTS_FNAME_ROOT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create human friendly JSON (.json) and data friendly JSON Lines (.jsonl) from current landcape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {OUT_DIR}\n",
    "\n",
    "!wget -O {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact https://landscape.cncf.io/data/items.json\n",
    "!ls -lh {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create human friendly file\n",
    "!jq . {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact > {CNCF_LANDSCAPE_FNAME_ROOT}.json\n",
    "!ls -lh {CNCF_LANDSCAPE_FNAME_ROOT}.json*\n",
    "!echo \"\\n*Yes* indeed, that's 2+ MB of whitespace!\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of JSON --> JSONL\n",
    "!jq  -c '.[]'  {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact >  {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "!ls -lh {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "!wc -l  {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Landscape: ~2200+ cards (cncf-landscape.jsonl) -->  ~180 CNCF Projects (cncf-projects.jsonl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lahF {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "!wc -l    {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "!echo \"\"\n",
    "\n",
    "!set -x && jq -c 'select(.relation == \"graduated\" or .relation == \"incubating\" or .relation == \"sandbox\")' {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl > {CNCF_PROJECTS_FNAME_ROOT}.jsonl \n",
    "\n",
    "!echo \"\"\n",
    "!ls -lahF {CNCF_PROJECTS_FNAME_ROOT}.jsonl\n",
    "!wc -l {CNCF_PROJECTS_FNAME_ROOT}.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame helpers: safe_set_index(), split_org_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_set_index(df:         pd.DataFrame, \n",
    "                   idx_wanted: list[str],\n",
    "                   sort:       bool = True,\n",
    "                   inplace:    bool = True) -> pd.DataFrame:\n",
    "    '''check to see if the index is already set, else, data loss as set_index can be destructive'''\n",
    "    \n",
    "    idx_existing = list(df.index.names)\n",
    "\n",
    "    if idx_wanted == idx_existing:\n",
    "        print(f'\\n*** WARNING: attempt to set index to what it already is thwarted! \\n')\n",
    "    else:\n",
    "        df.set_index(idx_wanted, verify_integrity=True, inplace=inplace)\n",
    "        print(f'\\t Index changed from {idx_existing} --> {list(df.index.names)}') \n",
    "\n",
    "    if sort:\n",
    "        df.sort_index(inplace=inplace)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_org_repo(df:      pd.DataFrame, \n",
    "                   colname: str,\n",
    "                   drop:    bool = False,\n",
    "                   newcol_org_name:  str = 'org_name',\n",
    "                   newcol_repo_name: str = 'repo_name') -> pd.DataFrame:\n",
    "    '''split_org_repo(df, colname) - org_name/repo_name --> org_name, repo_name'''\n",
    "    \n",
    "    if colname is None:\n",
    "        raise ValueError('split_org_repo: missing colname!')\n",
    "\n",
    "    # https://swdevnotes.com/python/2022/extract-data-from-json-in-pandas-dataframe/\n",
    "    # expand=True returns a dataframe  which we can rename columns on\n",
    "    \n",
    "    df_newcols = df[colname].copy().str.split(pat='/', n=1, expand=True)\n",
    "    df_newcols.rename(columns={0: newcol_org_name, 1: newcol_repo_name}, inplace=True)\n",
    "\n",
    "    if drop:\n",
    "        df.drop(colname, axis=1, inplace=True)\n",
    "\n",
    "    df = pd.concat([df,df_newcols], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame Helper: clean_dataframe(df, categorical_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clean: cncf-projects.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load .jsonl --> df_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pdb on\n",
    "file_path = f'{CNCF_PROJECTS_FNAME_ROOT}.jsonl'\n",
    "assert os.path.exists(file_path) and os.path.getsize(file_path) > 0, f\"File {file_path} does not exist or is empty.\"\n",
    "\n",
    "df_projects = pd.read_json(file_path, lines=True)\n",
    "print(df_projects.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cols += { subcategory, repo, org_name, repo_name }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out subcategory from path (category / subcategory)\n",
    "df_projects['subcategory'] = df_projects['path'].str.split('/').str[-1]\n",
    "\n",
    "# https://github.com/theOrg/theRepo --> repo := theOrg/theRepo\n",
    "df_projects['repo'] = df_projects['repo_url'].astype('string').str.removeprefix('https://github.com/')\n",
    "\n",
    "# theOrg/theRepo --> org_name := theOrg, repo_name := theRepo\n",
    "df_projects = split_org_repo(df_projects, 'repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN --> -1\n",
    "df_projects.stars = df_projects.stars.fillna(-1).astype('int64')\n",
    "df_projects.contributorsCount = df_projects.contributorsCount.fillna(-1).astype('int64')\n",
    "df_projects.enduser = df_projects.enduser.fillna(-1).astype('int64')\n",
    "\n",
    "# float64 --> int64\n",
    "df_projects.stars = df_projects.stars.astype('int64')\n",
    "df_projects.contributorsCount = df_projects.contributorsCount.astype('int64')\n",
    "df_projects.enduser = df_projects.enduser.astype('int64')\n",
    "\n",
    "# int64 --> bool\n",
    "df_projects.open_source = df_projects.open_source.astype('bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create smaller dataframe to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this overwrite's a global df, we should probably change this to not be destructive.\n",
    "\n",
    "# df2=df[['B','D','F']].rename({'B':'X','D':'Y','F':'Z'}, axis=1)\n",
    "\n",
    "# TODO: Remove\n",
    "df_projects_full = df_projects.copy()\n",
    "\n",
    "df_projects.reset_index(inplace=True)\n",
    "df_projects = df_projects[[\n",
    "                    'relation',\n",
    "                    'category',\n",
    "                    'subcategory',\n",
    "                    'id',\n",
    "                    'name',\n",
    "                    'flatName',\n",
    "                    'repo',\n",
    "                    'repos',\n",
    "                    'repo_name',\n",
    "                    'org_name',\n",
    "                    'contributorsCount',\n",
    "                    'commitsThisYear',\n",
    "                    'stars',\n",
    "                    'github_data',\n",
    "                    'extra',\n",
    "                    'industries',\n",
    "                    'headquarters',\n",
    "                    'image_data']].copy()\n",
    "\n",
    "df_projects.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### diff by using \"merge with indicator.\" \n",
    "\n",
    "The _merge column will be \"left_only\" for rows that are in df but not in df_dropna.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html#pandas-dataframe-merge\n",
    "\n",
    "---\n",
    "\n",
    "> _... \\<snip/\\> ..._\n",
    "> \n",
    "> **how{‘left’, ‘right’, ‘outer’, ‘inner’, ‘cross’}, default ‘inner’**\n",
    "> Type of merge to be performed.\n",
    "> \n",
    "> * left: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "> * right: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "> * outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "> * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "> * cross: creates the cartesian product from both frames, preserves the order of the left keys.\n",
    "> \n",
    "> _... \\<snip/\\> ..._\n",
    "> \n",
    ">  **indicator: _bool_ or _str_, default False**\n",
    ">  \n",
    "> If True, adds a column to the output DataFrame called “_merge” with information on the source of each row. The column can be given a different name by providing a string argument. \n",
    "> \n",
    "> The column will have a Categorical type with the value of \n",
    ">   * “left_only” for observations whose merge key only appears in the left DataFrame\n",
    ">   * “right_only” for observations whose merge key only appears in the right DataFrame\n",
    ">   * “both” if the observation’s merge key is found in both DataFrames.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows with embedded NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'before nulls removed: {df_projects.shape}')\n",
    "df_projects_dropna = df_projects.dropna().copy()\n",
    "print(f'after nulls removed: {df_projects_dropna.shape}')\n",
    "\n",
    "difference = df_projects[['name']].merge(df_projects_dropna[['name']], \n",
    "                                         how='outer', \n",
    "                                         indicator=True).loc[lambda x : x['_merge']=='left_only']\n",
    "\n",
    "print(f'*** {difference.shape[0]} rows with nulls excluded from analysis ***')\n",
    "display(df_projects.iloc[difference.index])\n",
    "\n",
    "# prune / trim\n",
    "df_projects = df_projects_dropna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical, numeric width, NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comparison purposes\n",
    "df_precleaned = df_projects.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Helpers: clean_dataframe(), compare_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "def clean_dataframe(df: pd.DataFrame, categorical_threshold: float = 0.05) -> pd.DataFrame:\n",
    "    # Infer better data types for object columns\n",
    "    df = df.infer_objects()\n",
    "\n",
    "    categorical_cols = []\n",
    "    # date_cols = []\n",
    "    dict_cols = []\n",
    "    list_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        print(f'processing col: {col}...')\n",
    "        if df[col].dtype in ['int64', 'float64']:\n",
    "            df[col] = pd.to_numeric(df[col], downcast='integer' if df[col].dtype == 'int64' else 'float')\n",
    "        elif df[col].dtype == 'object':\n",
    "            if isinstance(df[col].iloc[0], dict):\n",
    "                dict_cols.append(col)\n",
    "                continue\n",
    "            elif isinstance(df[col].iloc[0], list):\n",
    "                list_cols.append(col)\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                # TODO: handle date parsing\n",
    "                # try:\n",
    "                #     df[col] = pd.to_datetime(df[col])\n",
    "                #     date_cols.append(col)\n",
    "                # except ValueError:\n",
    "                #     pass\n",
    "\n",
    "                # try for categorical\n",
    "                if all(isinstance(i, (int, float, str)) for i in df[col]): \n",
    "                    num_unique_values = df[col].nunique()\n",
    "                    num_total_values = len(df[col])\n",
    "                    if num_unique_values / num_total_values < categorical_threshold:\n",
    "                        df[col] = df[col].astype('category')\n",
    "                        categorical_cols.append(col)\n",
    "\n",
    "\n",
    "    # Print summary of findings\n",
    "    print(f'Columns({len(df.columns)}) Summary:')\n",
    "    print(f'Categorical : {categorical_cols}')\n",
    "    # print(f'Date        : {date_cols}')\n",
    "    print(f'Dictionary  : {dict_cols}')\n",
    "    print(f'List        : {list_cols}')\n",
    "\n",
    "    # Create a visual summary using Altair\n",
    "    # for col in categorical_cols:\n",
    "    #     chart = alt.Chart(df).mark_bar().encode(\n",
    "    #         x=alt.X(col, type='nominal'),\n",
    "    #         y='count()',\n",
    "    #     )\n",
    "    #     chart.display()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(df1: pd.DataFrame, df2: pd.DataFrame):\n",
    "    # Calculate memory usage\n",
    "    memory_df1 = df1.memory_usage(deep=True)\n",
    "    memory_df2 = df2.memory_usage(deep=True)\n",
    "\n",
    "    # Calculate data types\n",
    "    dtype_df1 = df1.dtypes.astype(str)\n",
    "    dtype_df2 = df2.dtypes.astype(str)\n",
    "\n",
    "    # Create a new dataframe for comparison\n",
    "    comparison = pd.DataFrame({\n",
    "        'Original Dtype': dtype_df1,\n",
    "        'Cleaned Dtype': dtype_df2,\n",
    "        'Original Memory': memory_df1,\n",
    "        'Cleaned Memory': memory_df2\n",
    "    })\n",
    "\n",
    "    # Calculate memory reduction\n",
    "    comparison['Memory Reduction'] = comparison['Original Memory'] - comparison['Cleaned Memory']\n",
    "\n",
    "    # Print the comparison dataframe\n",
    "    print(comparison)\n",
    "\n",
    "    # Print total memory usage and reduction\n",
    "    total_reduction = comparison['Memory Reduction'].sum()\n",
    "    print(f\"\\nTotal memory usage of original dataframe: {memory_df1.sum()}\")\n",
    "    print(f\"Total memory usage of cleaned dataframe: {memory_df2.sum()}\")\n",
    "    print(f\"Total memory reduction: {total_reduction}\")\n",
    "\n",
    "    # Create a bar chart using Altair\n",
    "    comparison = comparison.reset_index().melt('index', var_name='Category', value_name='Value')\n",
    "    chart = alt.Chart(comparison).mark_bar().encode(\n",
    "        x='index:N',\n",
    "        y='Value:Q',\n",
    "        color='Category:N',\n",
    "        tooltip=['index:N', 'Value:Q', 'Category:N']\n",
    "    ).interactive()\n",
    "\n",
    "    # Display the chart\n",
    "    chart.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects_cleaned = clean_dataframe(df_projects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_dataframes(df_projects, df_projects_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects = df_projects_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Files: categories.txt, subcategories.txt, org_names.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_file(itemlist: list, fname: str, title: \"Unknown List\") -> None:\n",
    "    '''write list to file'''\n",
    "\n",
    "    print(f'{title}: {itemlist} ({fname})\\n')\n",
    "    \n",
    "    with open(fname, \"w\") as outfile:\n",
    "        outfile.write('\\n'.join(str(item) for item in itemlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories    = df_projects['category'].drop_duplicates().tolist()\n",
    "subcategories = df_projects['subcategory'].drop_duplicates().tolist()\n",
    "org_names     = df_projects['org_name'].drop_duplicates().tolist()\n",
    "\n",
    "list_to_file(categories,    f'{OUT_DIR}/categories.txt',    'CATEGORIES')\n",
    "list_to_file(subcategories, f'{OUT_DIR}/subcategories.txt', 'SUBCATEGORIES')\n",
    "list_to_file(org_names,     f'{OUT_DIR}/org_names.txt',     'ORG_NAMES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** INSERT GHARCHIVE DATASET HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#safe_set_index(df, ['relation', 'category', 'subcategory', 'id'])\n",
    "safe_set_index(df_projects, ['relation', 'category', 'subcategory', 'id'])\n",
    "\n",
    "df_numeric = df_projects.select_dtypes(include=['int64', 'int32', 'float64', 'float32'])\n",
    "df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation at level 0\n",
    "df_level0_relation = df_numeric.groupby(level=0).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 1\n",
    "df_level1_category = df_numeric.groupby(level=1).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 2\n",
    "df_level2_subcategory = df_numeric.groupby(level=2).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 3\n",
    "# df_level3 = df.groupby(level=3).sum()  # or .mean(), .count(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Aggregations at df_level0_relation\\n\\n{df_level0_relation}\\n')\n",
    "print(f'Aggregations at df_level1_category\\n\\n{df_level1_category}\\n')\n",
    "print(f'Aggregations at df_level2_subcategory\\n\\n{df_level2_subcategory}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'df.index.names: {df.index.names}\\n\\n')\n",
    "# #print(f'df.index.levels: {df.index.levels}\\n\\n')\n",
    "\n",
    "# for level in df.index.levels:\n",
    "#     print(f'level: {level}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sunbursts and Treemaps for { Contributors Count, Commits this year, Stars }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotly Imports & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# https://plotly.com/python/pandas-backend\n",
    "pd.options.plotting.backend = \"plotly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light to Dark Transition:\n",
    "# color_discrete_map={'sandbox': '#ADD8E6', 'incubating': '#87CEEB', 'graduated': '#000080'}\n",
    "#\n",
    "# sandbox: Light Blue (#ADD8E6)\n",
    "# incubating: Medium Blue (#87CEEB)\n",
    "# graduated: Dark Blue (#000080)\n",
    "\n",
    "# Warm to Cool Transition:\n",
    "# color_discrete_map={'sandbox': '#FFA500', 'incubating': '#FFD700', 'graduated': '#008000'}\n",
    "#\n",
    "# sandbox: Orange (#FFA500)\n",
    "# incubating: Yellow (#FFD700)\n",
    "# graduated: Green (#008000)\n",
    "\n",
    "# Warm to Cool 2 Transition:\n",
    "# color_discrete_map={'sandbox': '#FFD700', 'incubating': '#87CEEB', 'graduated': '#008000'}\n",
    "#\n",
    "# sandbox: Yellow (#FFD700)\n",
    "# incubating: Medium Blue (#87CEEB)\n",
    "# graduated: Green (#008000)\n",
    "\n",
    "color_discrete_map_pastel = {\n",
    "    \"App Definition and Development\": \"#a2cffe\",\n",
    "    \"Observability and Analysis\": \"#8efac1\",\n",
    "    \"Orchestration & Management\": \"#fc9d9a\",\n",
    "    \"Platform\": \"#c0eb75\",\n",
    "    \"Provisioning\": \"#f2a2e8\",\n",
    "    \"Runtime\": \"#fffe7a\",\n",
    "    \"Serverless\": \"#d3d3d3\"\n",
    "}\n",
    "\n",
    "color_discrete_map1 = {\n",
    "    \"App Definition and Development\": \"#264653\",\n",
    "    \"Observability and Analysis\": \"#2a9d8f\",\n",
    "    \"Orchestration & Management\": \"#e9c46a\",\n",
    "    \"Platform\": \"#f4a261\",\n",
    "    \"Provisioning\": \"#e76f51\",\n",
    "    \"Runtime\": \"#6d6875\",\n",
    "    \"Serverless\": \"#fca311\"\n",
    "}\n",
    "\n",
    "\n",
    "color_discrete_map2 = {\n",
    "    \"App Definition and Development\": \"#003f5c\",\n",
    "    \"Observability and Analysis\": \"#58508d\",\n",
    "    \"Orchestration & Management\": \"#bc5090\",\n",
    "    \"Platform\": \"#ff6361\",\n",
    "    \"Provisioning\": \"#ffa600\",\n",
    "    \"Runtime\": \"#2f4b7c\",\n",
    "    \"Serverless\": \"#665191\"\n",
    "}\n",
    "color_discrete_map3 = {\n",
    "    \"App Definition and Development\": \"#165aa7\",\n",
    "    \"Observability and Analysis\": \"#cb495c\",\n",
    "    \"Orchestration & Management\": \"#bb60d5\",\n",
    "    \"Platform\": \"#f47915\",\n",
    "    \"Provisioning\": \"#06ab54\",\n",
    "    \"Runtime\": \"#002070\",\n",
    "    \"Serverless\": \"#b27d12\"\n",
    "}\n",
    "\n",
    "color_discrete_map4 = {\n",
    "    \"App Definition and Development\": \"#1f77b4\",\n",
    "    \"Observability and Analysis\": \"#ff7f0e\",\n",
    "    \"Orchestration & Management\": \"#2ca02c\",\n",
    "    \"Platform\": \"#d62728\",\n",
    "    \"Provisioning\": \"#9467bd\",\n",
    "    \"Runtime\": \"#8c564b\",\n",
    "    \"Serverless\": \"#e377c2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(plotly_func,\n",
    "                  df,\n",
    "                  values=None,\n",
    "                  height: int = 1200,\n",
    "                  width: int = 1200,\n",
    "                  title: str = 'Missing Title',\n",
    "                  path=['category', 'subcategory', 'id'],  # TODO: add 4th level: repo_name\n",
    "                  color='category',\n",
    "                  color_discrete_map=color_discrete_map2,\n",
    "                  branchvalues: str = None) -> go.Figure:\n",
    "    \n",
    "    fig = plotly_func(data_frame=df,\n",
    "                      values=values,\n",
    "                      height=height,\n",
    "                      width=width,\n",
    "                      title=title,\n",
    "                      path=path,\n",
    "                      color=color,\n",
    "                      color_discrete_map=color_discrete_map,\n",
    "                      branchvalues='total')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_sunburst(df, **kwargs) -> go.Figure:\n",
    "    return create_figure(px.sunburst, df, **kwargs)\n",
    "\n",
    "\n",
    "def create_treemap(df, **kwargs) -> go.Figure:\n",
    "    return create_figure(px.treemap, df, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Figures (Sunbursts, Treemaps) w/ plotly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING WARNING WARNING: Until the full repo sets are included, these are **HIGHLY INACCURATE**!\n",
    "\n",
    "Presently they **ONLY** contain contributions for the singular repo listed in the landscape, instead of the full set of repos.  \n",
    "\n",
    "For example, open-telemetry is not just one single repo (the Java Agent), nor is Kubernetes simply https://github.com/kubernetes/kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make index columns accessible for charting as normal columns\n",
    "df_reset = df_projects.reset_index()\n",
    "\n",
    "figs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sunb_contributorsCount = create_sunburst(df_reset, values='contributorsCount', title='sunburst: 🪴 Contributor Count (NOT UNIQUE ACROSS PROJECTS!) 🪴')\n",
    "sunb_commitsThisYear   = create_sunburst(df_reset, values='commitsThisYear',   title='sunburst: 📄 Commits This Year 📄')\n",
    "sunb_stars             = create_sunburst(df_reset, values='stars',             title='sunburst: ⭐ Stars ⭐')\n",
    "\n",
    "tree_contributorsCount = create_treemap(df_reset, values='contributorsCount',  title='treemap: 🪴 Contributor Count (NOT UNIQUE ACROSS PROJECTS!) 🪴')\n",
    "tree_commitsThisYear   = create_treemap(df_reset, values='commitsThisYear',    title='treemap: 📄 Commits This Year 📄')\n",
    "tree_stars             = create_treemap(df_reset, values='stars',              title='treemap: ⭐ Stars ⭐')\n",
    "\n",
    "figs['sunb_contributorsCount'] = sunb_contributorsCount\n",
    "figs['sunb_commitsThisYear']   = sunb_commitsThisYear\n",
    "figs['sunb_stars']             = sunb_stars\n",
    "\n",
    "figs['tree_contributorsCount'] = tree_contributorsCount\n",
    "figs['tree_commitsThisYear']   = tree_commitsThisYear\n",
    "figs['tree_stars']             = tree_stars\n",
    "\n",
    "for key, fig in figs.items():\n",
    "    file_name = f'{OUT_DIR}/fig_{key}.svg'\n",
    "    fig.write_image(file_name, format='svg')\n",
    "\n",
    "    # Emit raw markdown for image description\n",
    "    markdown = f\"![Image description]({file_name})\"\n",
    "    print(f\"```{markdown}```\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sunbursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_contributorsCount'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_stars'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_commitsThisYear'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Treemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_contributorsCount'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_stars'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_commitsThisYear'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images for github rendering\n",
    "\n",
    "![Image description](generated/fig_sunb_contributorsCount.svg)\n",
    "![Image description](generated/fig_sunb_commitsThisYear.svg)\n",
    "![Image description](generated/fig_sunb_stars.svg)\n",
    "![Image description](generated/fig_tree_contributorsCount.svg)\n",
    "![Image description](generated/fig_tree_commitsThisYear.svg)\n",
    "![Image description](generated/fig_tree_stars.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Per TAG views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(f'index.names: {df.index.names}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_set_index(df, ['category', 'subcategory', 'relation', 'id'])\n",
    "\n",
    "df.info()\n",
    "print(f'index.names: {df.index.names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug - just Observability TAG Projects\n",
    "#repos_by_relation = df.query(\"`category` == 'Observability and Analysis'\")[['relation','repo', 'name']].copy()\n",
    "\n",
    "repos_by_relation = df.copy().reset_index()\n",
    "\n",
    "repos_by_relation.groupby('relation')['repo'].agg(lambda x: list(x)).to_dict()\n",
    "\n",
    "safe_set_index(repos_by_relation, idx_wanted=['relation', 'name'])\n",
    "repos_by_relation.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduated_single_repos  = repos_by_relation.loc['graduated', :]['repo'].tolist()\n",
    "incubating_single_repos = repos_by_relation.loc['incubating', :]['repo'].tolist()\n",
    "sandbox_single_repos    = repos_by_relation.loc['sandbox', :]['repo'].tolist()\n",
    "\n",
    "display(graduated_single_repos, incubating_single_repos, sandbox_single_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch project release data from GitHub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from datetime import datetime, timezone\n",
    "from github import Github, GithubException\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "def fetch_repo_data(token: str, \n",
    "                    repo_list: List[str], \n",
    "                    since: datetime=None, \n",
    "                    json_file: str=None, \n",
    "                    csv_file: str=None, \n",
    "                    state_file: str=None) -> pd.DataFrame:\n",
    "\n",
    "    # Initialize DataFrame\n",
    "    # df = pd.DataFrame(columns=[\n",
    "    #     'repo_name', 'release_name', 'release_date', \n",
    "    #     'language', 'release_notes'\n",
    "    # ])\n",
    "\n",
    "    df = pd.DataFrame(columns=['repo_name', 'release_name', 'release_date', 'language'])\n",
    "\n",
    "    # Initialize GitHub client\n",
    "    g = Github(token)\n",
    "\n",
    "    # Initialize loop state\n",
    "    if state_file:\n",
    "        try:\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            state = {'i': 0, 'repos_done': []}\n",
    "    else:\n",
    "        state = {'i': 0, 'repos_done': []}\n",
    "\n",
    "    # Loop over repositories\n",
    "    while state['i'] < len(repo_list):\n",
    "        repo_str = repo_list[state['i']]\n",
    "\n",
    "        if repo_str in state['repos_done']:\n",
    "            print(f\"Skipping: {repo_str}\")\n",
    "            state['i'] += 1\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                repo = g.get_repo(repo_str)\n",
    "                break\n",
    "            except GithubException as e:\n",
    "                if e.status == 404:\n",
    "                    print(f\"Repository {repo_str} not found\")\n",
    "                    break\n",
    "                elif e.status == 429:\n",
    "                    print(f\"Rate limit exceeded, waiting for {e.headers['Retry-After']} seconds...\")\n",
    "                    time.sleep(int(e.headers['Retry-After']))\n",
    "                else:\n",
    "                    print(f\"Error getting repository {repo_str}: {e}\")\n",
    "                    break\n",
    "\n",
    "        if not repo:\n",
    "            state['i'] += 1\n",
    "            continue\n",
    "\n",
    "        #\n",
    "        # Get all releases\n",
    "        #\n",
    "        releases = repo.get_releases()\n",
    "        language = repo.language\n",
    "\n",
    "        for release in releases:\n",
    "            if since is None or release.created_at >= since:\n",
    "                df = pd.concat([df, pd.DataFrame({\n",
    "                    'repo_name': [repo_str],\n",
    "                    'release_name': [release.title],\n",
    "                    'release_date': [str(release.published_at)],\n",
    "                    'language': [language],\n",
    "                    #'release_notes': [release.body]\n",
    "                })])\n",
    "                print(f\"Added {release.published_at}, {repo_str}::{release.title}  \")\n",
    "\n",
    "        # Save state\n",
    "        if state_file:\n",
    "            state['repos_done'].append(repo_str)\n",
    "            with open(state_file, 'w') as f:\n",
    "                json.dump(state, f, indent=4)\n",
    "\n",
    "        state['i'] += 1\n",
    "\n",
    "    #print (releases)\n",
    "    \n",
    "    # Save as CSV\n",
    "    if csv_file:\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "    # Save as JSON\n",
    "    if json_file:\n",
    "        df.to_json(json_file, orient='records', lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_one(token, since_date, level, repos):\n",
    "\n",
    "    json_file=f'out/{level}-github-releases.json' \n",
    "    csv_file=f'out/{level}-github-releases.csv'\n",
    "    state_file=f'out/.nukeme_state_file_{level}'\n",
    "    \n",
    "    print(f\"Fetching {len(repos)} repositories for {level} projects\")\n",
    "    \n",
    "    releases = fetch_repo_data( token, \n",
    "                                repos, \n",
    "                                since=since_date,\n",
    "                                json_file=json_file,\n",
    "                                csv_file=csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ['GITHUB_TOKEN']\n",
    "since_date = datetime(2022, 11, 7, tzinfo=timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-graduated', graduated_single_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-incubating', incubating_single_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-sandbox', sandbox_single_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipympl\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_releases_timeline(releases: pd.DataFrame):\n",
    "#     fig = px.timeline(releases, x_start=\"release_date\", x_end=\"release_date\", y=\"repo_name\", color=\"language\", title=\"GitHub Releases Timeline\")\n",
    "#     fig.update_yaxes(autorange=\"reversed\")\n",
    "#     fig.show()\n",
    "\n",
    "# def plot_releases_scatter_simple(releases: pd.DataFrame):\n",
    "#     # Filter releases by year\n",
    "#     releases_2023 = releases[releases['release_date'].dt.year == 2023]\n",
    "\n",
    "#     # Create scatter plot\n",
    "#     fig = px.scatter(releases_2023, x=\"release_date\", y=\"repo_name\", color=\"language\")\n",
    "#     fig.update_yaxes(autorange=\"reversed\")\n",
    "#     fig.show()\n",
    "\n",
    "def plot_releases_scatter(releases: pd.DataFrame, title: str=None):\n",
    "    # # Filter releases by year\n",
    "    # releases_2023 = releases[releases['release_date'].dt.year == 2023]\n",
    "\n",
    "    if title is None:\n",
    "        title = \"GitHub Releases Timeline\"\n",
    "\n",
    "    # Group releases by organization\n",
    "    releases['organization'] = releases['repo_name'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = px.scatter(releases, x=\"release_date\", y=\"repo_name\", color=\"organization\", symbol=\"language\", title=\"Project Releases\")\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    \n",
    "    fig.update_layout(showlegend=True,\n",
    "                      autosize=True,\n",
    "                      width=1000)\n",
    "                    #   height=2500,\n",
    "                    #   )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def json_to_csv(json_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load a JSON file into a pandas DataFrame and save it as a CSV file with the same name.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(json_file_path, lines=True)\n",
    "    \n",
    "    csv_file_path = os.path.splitext(json_file_path)[0] + '.csv'\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['graduated', 'incubating', 'sandbox']:\n",
    "    json_to_csv(f'out/cncf-{level}-github-releases.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_releases_from_csv(csv_file: str, title: str) -> None:\n",
    "    csv_file =f'out/cncf-{level}-github-releases.csv'\n",
    "    if os.path.exists(csv_file):\n",
    "        df_releases = pd.read_csv(csv_file)\n",
    "        df_releases.release_date = pd.to_datetime(df_releases.release_date)\n",
    "        \n",
    "        plot_releases_scatter(df_releases, title)\n",
    "    else:\n",
    "        print(f\"CSV file {csv_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for level in ['graduated', 'incubating', 'sandbox']:\n",
    "    plot_releases_from_csv(f'out/cncf-{level}-github-releases.csv', f'Releases: {level}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# plot_releases_scatter(f'out/cncf-all-github-releases.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "releases_by_repo = releases[['repo_name', 'release_date']].groupby('repo_name').count()\n",
    "releases_by_repo.to_csv('cncf_releases_by_repo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

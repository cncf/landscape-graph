{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNCF Landscape: {Category, Subcategory, Project } --> Stars, Commits, Contributors, Activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On JupyterLab Shell Integration(s) and SList \n",
    "\n",
    "More Info: _http://safaribooksonline.com/blog/2014/02/12/using-shell-commands-effectively-ipython_\n",
    "\n",
    "SList instances can be used like a regular list, but they provide several methods that are useful when working with shell output. The main properties available in an SList instance are:\n",
    "\n",
    "\n",
    "\n",
    "* `.s` returns the elements joined together by spaces. \n",
    "  * _This is useful for building command lines that take many arguments in a single invocation._\n",
    "* `.n` returns the elements joined together by a newline. \n",
    "  * _Use this when you need the original output unmodified._\n",
    "* `.p` returns the elements as path objects, if they are filenames.\\\n",
    "  * _Use this when doing more advanced path manipulation_\n",
    "\n",
    "In addition, SList instances support `grep()` and `fields()` methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On AI Ingtegration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- ------------\n",
      "aiohttp                            3.8.6\n",
      "aiosignal                          1.3.1\n",
      "aiosqlite                          0.19.0\n",
      "altair                             5.1.2\n",
      "annotated-types                    0.6.0\n",
      "anyio                              3.7.1\n",
      "appnope                            0.1.3\n",
      "argon2-cffi                        23.1.0\n",
      "argon2-cffi-bindings               21.2.0\n",
      "arrow                              1.3.0\n",
      "astroid                            3.0.1\n",
      "asttokens                          2.4.1\n",
      "async-lru                          2.0.4\n",
      "async-timeout                      4.0.3\n",
      "attrs                              23.1.0\n",
      "autopep8                           2.0.4\n",
      "Babel                              2.13.1\n",
      "backoff                            2.2.1\n",
      "beautifulsoup4                     4.12.2\n",
      "bleach                             6.1.0\n",
      "bokeh                              3.3.1\n",
      "botocore                           1.32.4\n",
      "bqplot                             0.12.42\n",
      "cachetools                         5.3.2\n",
      "certifi                            2023.11.17\n",
      "cffi                               1.16.0\n",
      "charset-normalizer                 3.3.2\n",
      "click                              8.1.7\n",
      "cloudpickle                        3.0.0\n",
      "comm                               0.2.0\n",
      "contourpy                          1.2.0\n",
      "cryptography                       41.0.5\n",
      "cycler                             0.12.1\n",
      "dask                               2023.11.0\n",
      "dask_labextension                  7.0.0\n",
      "dataclasses-json                   0.6.2\n",
      "debugpy                            1.8.0\n",
      "decorator                          5.1.1\n",
      "deepmerge                          1.1.0\n",
      "defusedxml                         0.7.1\n",
      "Deprecated                         1.2.14\n",
      "dill                               0.3.7\n",
      "distributed                        2023.11.0\n",
      "docstring-to-markdown              0.13\n",
      "executing                          2.0.1\n",
      "faiss-cpu                          1.7.4\n",
      "fastjsonschema                     2.19.0\n",
      "flake8                             6.1.0\n",
      "fonttools                          4.45.0\n",
      "fqdn                               1.5.1\n",
      "frozenlist                         1.4.0\n",
      "fsspec                             2023.10.0\n",
      "gast                               0.4.0\n",
      "google-api-core                    2.14.0\n",
      "google-api-python-client           2.108.0\n",
      "google-auth                        2.23.4\n",
      "google-auth-httplib2               0.1.1\n",
      "google-auth-oauthlib               1.1.0\n",
      "googleapis-common-protos           1.61.0\n",
      "gql                                3.4.1\n",
      "graphql-core                       3.2.3\n",
      "httplib2                           0.22.0\n",
      "idna                               3.4\n",
      "importlib-metadata                 6.8.0\n",
      "inflect                            7.0.0\n",
      "ipycytoscape                       1.3.3\n",
      "ipydatagrid                        1.2.0\n",
      "ipykernel                          6.26.0\n",
      "ipython                            8.17.2\n",
      "ipywidgets                         8.1.1\n",
      "isoduration                        20.11.0\n",
      "isort                              5.12.0\n",
      "jedi                               0.19.1\n",
      "Jinja2                             3.1.2\n",
      "jmespath                           1.0.1\n",
      "jp                                 0.2.4\n",
      "jq                                 1.6.0\n",
      "json5                              0.9.14\n",
      "jsonpatch                          1.33\n",
      "jsonpath-ng                        1.6.0\n",
      "jsonpointer                        2.4\n",
      "jsonschema                         4.20.0\n",
      "jsonschema-specifications          2023.11.1\n",
      "jupyter_ai                         2.6.0\n",
      "jupyter_ai_magics                  2.6.0\n",
      "jupyter_client                     8.6.0\n",
      "jupyter_core                       5.5.0\n",
      "jupyter-events                     0.9.0\n",
      "jupyter-lsp                        2.2.0\n",
      "jupyter-resource-usage             1.0.1\n",
      "jupyter_server                     2.10.1\n",
      "jupyter_server_proxy               4.1.0\n",
      "jupyter_server_terminals           0.4.4\n",
      "jupyterlab                         4.0.9\n",
      "jupyterlab_favorites               3.2.1\n",
      "jupyterlab-lsp                     5.0.0\n",
      "jupyterlab-pygments                0.2.2\n",
      "jupyterlab_recents                 3.3.0\n",
      "jupyterlab_server                  2.25.2\n",
      "jupyterlab-spellchecker            0.8.4\n",
      "jupyterlab-visualpython            3.0.1\n",
      "jupyterlab-widgets                 3.0.9\n",
      "jupyterthemes                      0.20.0\n",
      "jupytext                           1.15.2\n",
      "kaleido                            0.2.1\n",
      "kiwisolver                         1.4.5\n",
      "langchain                          0.0.318\n",
      "langsmith                          0.0.66\n",
      "lesscpy                            0.15.1\n",
      "locket                             1.0.0\n",
      "lz4                                4.3.2\n",
      "markdown-it-py                     3.0.0\n",
      "MarkupSafe                         2.1.3\n",
      "marshmallow                        3.20.1\n",
      "matplotlib                         3.8.2\n",
      "matplotlib-inline                  0.1.6\n",
      "mccabe                             0.7.0\n",
      "mdit-py-plugins                    0.4.0\n",
      "mdurl                              0.1.2\n",
      "mistune                            3.0.2\n",
      "msgpack                            1.0.7\n",
      "multidict                          6.0.4\n",
      "mypy-extensions                    1.0.0\n",
      "nbclient                           0.7.4\n",
      "nbconvert                          7.11.0\n",
      "nbformat                           5.9.2\n",
      "nest-asyncio                       1.5.8\n",
      "notebook                           7.0.6\n",
      "notebook_shim                      0.2.3\n",
      "numpy                              1.26.2\n",
      "oauthlib                           3.2.2\n",
      "openai                             0.28.1\n",
      "opentelemetry-api                  1.21.0\n",
      "opentelemetry-sdk                  1.21.0\n",
      "opentelemetry-semantic-conventions 0.42b0\n",
      "overrides                          7.4.0\n",
      "packaging                          23.2\n",
      "pandas                             2.1.3\n",
      "pandocfilters                      1.5.0\n",
      "parso                              0.8.3\n",
      "partd                              1.4.1\n",
      "pexpect                            4.8.0\n",
      "Pillow                             10.1.0\n",
      "pip                                23.3.1\n",
      "platformdirs                       4.0.0\n",
      "plotly                             5.18.0\n",
      "pluggy                             1.3.0\n",
      "ply                                3.11\n",
      "prometheus-client                  0.19.0\n",
      "prompt-toolkit                     3.0.41\n",
      "protobuf                           4.25.1\n",
      "psutil                             5.9.6\n",
      "psycopg2-binary                    2.9.9\n",
      "ptyprocess                         0.7.0\n",
      "pure-eval                          0.2.2\n",
      "py2vega                            0.6.1\n",
      "pyarrow                            14.0.1\n",
      "pyarrow-hotfix                     0.5\n",
      "pyasn1                             0.5.1\n",
      "pyasn1-modules                     0.3.0\n",
      "pycodestyle                        2.11.1\n",
      "pycparser                          2.21\n",
      "pydantic                           2.5.1\n",
      "pydantic_core                      2.14.3\n",
      "pydocstyle                         6.3.0\n",
      "pyflakes                           3.1.0\n",
      "PyGithub                           2.1.1\n",
      "Pygments                           2.17.1\n",
      "PyJWT                              2.8.0\n",
      "pylint                             3.0.2\n",
      "pyls-isort                         0.2.2\n",
      "pylsp-rope                         0.1.11\n",
      "PyNaCl                             1.5.0\n",
      "pyparsing                          3.1.1\n",
      "python-dateutil                    2.8.2\n",
      "python-dotenv                      1.0.0\n",
      "python-json-logger                 2.0.7\n",
      "python-lsp-jsonrpc                 1.1.2\n",
      "python-lsp-server                  1.9.0\n",
      "pytoolconfig                       1.2.6\n",
      "pytz                               2023.3.post1\n",
      "PyYAML                             6.0.1\n",
      "pyzmq                              25.1.1\n",
      "referencing                        0.31.0\n",
      "regex                              2023.10.3\n",
      "requests                           2.31.0\n",
      "requests-oauthlib                  1.3.1\n",
      "requests-toolbelt                  0.10.1\n",
      "rfc3339-validator                  0.1.4\n",
      "rfc3986-validator                  0.1.1\n",
      "rope                               1.11.0\n",
      "rpds-py                            0.13.1\n",
      "rsa                                4.9\n",
      "Send2Trash                         1.8.2\n",
      "setuptools                         65.5.0\n",
      "simpervisor                        1.0.0\n",
      "six                                1.16.0\n",
      "sniffio                            1.3.0\n",
      "snowballstemmer                    2.2.0\n",
      "sortedcontainers                   2.4.0\n",
      "soupsieve                          2.5\n",
      "spectate                           1.0.1\n",
      "sqlacodegen-v2                     0.1.4\n",
      "SQLAlchemy                         2.0.23\n",
      "stack-data                         0.6.3\n",
      "tabulate                           0.9.0\n",
      "tblib                              3.0.0\n",
      "tenacity                           8.2.3\n",
      "terminado                          0.18.0\n",
      "tiktoken                           0.5.1\n",
      "tinycss2                           1.2.1\n",
      "toml                               0.10.2\n",
      "tomli                              2.0.1\n",
      "tomlkit                            0.12.3\n",
      "toolz                              0.12.0\n",
      "tornado                            6.3.3\n",
      "tqdm                               4.66.1\n",
      "traitlets                          5.13.0\n",
      "traittypes                         0.2.1\n",
      "types-python-dateutil              2.8.19.14\n",
      "typing_extensions                  4.8.0\n",
      "typing-inspect                     0.9.0\n",
      "tzdata                             2023.3\n",
      "ujson                              5.8.0\n",
      "uri-template                       1.3.0\n",
      "uritemplate                        4.1.1\n",
      "urllib3                            1.26.18\n",
      "voila                              0.5.5\n",
      "wcwidth                            0.2.11\n",
      "webcolors                          1.13\n",
      "webencodings                       0.5.1\n",
      "websocket-client                   1.6.4\n",
      "websockets                         10.4\n",
      "wget                               3.2\n",
      "whatthepatch                       1.0.5\n",
      "widgetsnbextension                 4.0.9\n",
      "wrapt                              1.16.0\n",
      "xyzservices                        2023.10.1\n",
      "yapf                               0.40.2\n",
      "yarl                               1.9.3\n",
      "zict                               3.0.0\n",
      "zipp                               3.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyter_ai_magics extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyter_ai_magics\n"
     ]
    }
   ],
   "source": [
    "%load_ext jupyter_ai_magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `ai21:j1-large`, `ai21:j1-grande`, `ai21:j1-jumbo`, `ai21:j1-grande-instruct`, `ai21:j2-large`, `ai21:j2-grande`, `ai21:j2-jumbo`, `ai21:j2-grande-instruct`, `ai21:j2-jumbo-instruct` |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock:amazon.titan-text-express-v1`, `bedrock:ai21.j2-ultra-v1`, `bedrock:ai21.j2-mid-v1`, `bedrock:cohere.command-text-v14` |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `bedrock-chat:anthropic.claude-v1`, `bedrock-chat:anthropic.claude-v2`, `bedrock-chat:anthropic.claude-instant-v1` |\n",
       "| `anthropic` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic:claude-v1`, `anthropic:claude-v1.0`, `anthropic:claude-v1.2`, `anthropic:claude-2`, `anthropic:claude-2.0`, `anthropic:claude-instant-v1`, `anthropic:claude-instant-v1.0`, `anthropic:claude-instant-v1.2` |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `anthropic-chat:claude-v1`, `anthropic-chat:claude-v1.0`, `anthropic-chat:claude-v1.2`, `anthropic-chat:claude-2`, `anthropic-chat:claude-2.0`, `anthropic-chat:claude-instant-v1`, `anthropic-chat:claude-instant-v1.0`, `anthropic-chat:claude-instant-v1.2` |\n",
       "| `azure-chat-openai` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `cohere:medium`, `cohere:xlarge` |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | `gpt4all:ggml-gpt4all-j-v1.2-jazzy`, `gpt4all:ggml-gpt4all-j-v1.3-groovy`, `gpt4all:ggml-gpt4all-l13b-snoozy`, `gpt4all:mistral-7b-openorca.Q4_0`, `gpt4all:mistral-7b-instruct-v0.1.Q4_0`, `gpt4all:gpt4all-falcon-q4_0`, `gpt4all:wizardlm-13b-v1.2.Q4_0`, `gpt4all:nous-hermes-llama2-13b.Q4_0`, `gpt4all:gpt4all-13b-snoozy-q4_0`, `gpt4all:mpt-7b-chat-merges-q4_0`, `gpt4all:orca-mini-3b-gguf2-q4_0`, `gpt4all:starcoder-q4_0`, `gpt4all:rift-coder-v0-7b-q4_0`, `gpt4all:em_german_mistral_v01.Q4_0` |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai:text-davinci-003`, `openai:text-davinci-002`, `openai:text-curie-001`, `openai:text-babbage-001`, `openai:text-ada-001`, `openai:davinci`, `openai:curie`, `openai:babbage`, `openai:ada` |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai-chat:gpt-3.5-turbo`, `openai-chat:gpt-3.5-turbo-16k`, `openai-chat:gpt-3.5-turbo-0301`, `openai-chat:gpt-3.5-turbo-0613`, `openai-chat:gpt-3.5-turbo-16k-0613`, `openai-chat:gpt-4`, `openai-chat:gpt-4-0314`, `openai-chat:gpt-4-0613`, `openai-chat:gpt-4-32k`, `openai-chat:gpt-4-32k-0314`, `openai-chat:gpt-4-32k-0613` |\n",
       "| `openai-chat-new` | `OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | `openai-chat-new:gpt-3.5-turbo`, `openai-chat-new:gpt-3.5-turbo-16k`, `openai-chat-new:gpt-3.5-turbo-0301`, `openai-chat-new:gpt-3.5-turbo-0613`, `openai-chat-new:gpt-3.5-turbo-16k-0613`, `openai-chat-new:gpt-4`, `openai-chat-new:gpt-4-0314`, `openai-chat-new:gpt-4-0613`, `openai-chat-new:gpt-4-32k`, `openai-chat-new:gpt-4-32k-0314`, `openai-chat-new:gpt-4-32k-0613` |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:text-davinci-003` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:cohere.command-text-v14\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:anthropic.claude-v1\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "\n",
       "anthropic\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic:claude-v1\n",
       "* anthropic:claude-v1.0\n",
       "* anthropic:claude-v1.2\n",
       "* anthropic:claude-2\n",
       "* anthropic:claude-2.0\n",
       "* anthropic:claude-instant-v1\n",
       "* anthropic:claude-instant-v1.0\n",
       "* anthropic:claude-instant-v1.2\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable ANTHROPIC_API_KEY (not set)\n",
       "* anthropic-chat:claude-v1\n",
       "* anthropic-chat:claude-v1.0\n",
       "* anthropic-chat:claude-v1.2\n",
       "* anthropic-chat:claude-2\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-instant-v1\n",
       "* anthropic-chat:claude-instant-v1.0\n",
       "* anthropic-chat:claude-instant-v1.2\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable COHERE_API_KEY (not set)\n",
       "* cohere:medium\n",
       "* cohere:xlarge\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "openai\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai:text-davinci-003\n",
       "* openai:text-davinci-002\n",
       "* openai:text-curie-001\n",
       "* openai:text-babbage-001\n",
       "* openai:text-ada-001\n",
       "* openai:davinci\n",
       "* openai:curie\n",
       "* openai:babbage\n",
       "* openai:ada\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-0314\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0314\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "\n",
       "openai-chat-new\n",
       "Requires environment variable OPENAI_API_KEY (not set)\n",
       "* openai-chat-new:gpt-3.5-turbo\n",
       "* openai-chat-new:gpt-3.5-turbo-16k\n",
       "* openai-chat-new:gpt-3.5-turbo-0301\n",
       "* openai-chat-new:gpt-3.5-turbo-0613\n",
       "* openai-chat-new:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat-new:gpt-4\n",
       "* openai-chat-new:gpt-4-0314\n",
       "* openai-chat-new:gpt-4-0613\n",
       "* openai-chat-new:gpt-4-32k\n",
       "* openai-chat-new:gpt-4-32k-0314\n",
       "* openai-chat-new:gpt-4-32k-0613\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deployment.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:text-davinci-003\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: %%ai [OPTIONS] MODEL_ID\n",
      "\n",
      "  Invokes a language model identified by MODEL_ID, with the prompt being\n",
      "  contained in all lines after the first. Both local model IDs and global\n",
      "  model IDs (with the provider ID explicitly prefixed, followed by a colon)\n",
      "  are accepted.\n",
      "\n",
      "  To view available language models, please run `%ai list`.\n",
      "\n",
      "Options:\n",
      "  -f, --format [code|html|image|json|markdown|math|md|text]\n",
      "                                  IPython display to use when rendering\n",
      "                                  output. [default=\"markdown\"]\n",
      "  -r, --reset                     Clears the conversation transcript used when\n",
      "                                  interacting with an OpenAI chat model\n",
      "                                  provider. Does nothing with other providers.\n",
      "  -n, --region-name TEXT          AWS region name, e.g. 'us-east-1'. Required\n",
      "                                  for SageMaker provider; does nothing with\n",
      "                                  other providers.\n",
      "  -q, --request-schema TEXT       The JSON object the endpoint expects, with\n",
      "                                  the prompt being substituted into any value\n",
      "                                  that matches the string literal '<prompt>'.\n",
      "                                  Required for SageMaker provider; does\n",
      "                                  nothing with other providers.\n",
      "  -p, --response-path TEXT        A JSONPath string that retrieves the\n",
      "                                  language model's output from the endpoint's\n",
      "                                  JSON response. Required for SageMaker\n",
      "                                  provider; does nothing with other providers.\n",
      "  -m, --model-parameters TEXT     A JSON value that specifies extra values\n",
      "                                  that will be passed to the model. The\n",
      "                                  accepted value parsed to a dict, unpacked\n",
      "                                  and passed as-is to the provider class.\n",
      "  --help                          Show this message and exit.\n",
      "------------------------------------------------------------------------------\n",
      "Usage: %ai [OPTIONS] COMMAND [ARGS]...\n",
      "\n",
      "  Invokes a subcommand.\n",
      "\n",
      "Options:\n",
      "  --help  Show this message and exit.\n",
      "\n",
      "Commands:\n",
      "  delete    Delete an alias. See `%ai delete --help` for options.\n",
      "  error     Explains the most recent error.\n",
      "  help      Show this message and exit.\n",
      "  list      List language models. See `%ai list --help` for options.\n",
      "  register  Register a new alias. See `%ai register --help` for options.\n",
      "  update    Update the target of an alias. See `%ai update --help` for\n",
      "            options.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%ai /help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base imports and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter Kernel (venv): /Users/matt/gh/cncf/landscape-graph/.venv-ipynb/bin/python\n",
      "Output Location:       generated  (.json, .jsonl, .csv, .md, ...)\n",
      "Output Landscape root: generated/cncf-landscape-*\n",
      "Output Projects  root: generated/cncf-projects-*\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OUT_DIR='generated'\n",
    "CNCF_LANDSCAPE_FNAME_BASE='cncf-landscape'\n",
    "CNCF_LANDSCAPE_FNAME_ROOT=f'{OUT_DIR}/{CNCF_LANDSCAPE_FNAME_BASE}'\n",
    "\n",
    "CNCF_PROJECTS_FNAME_BASE=f'cncf-projects'\n",
    "CNCF_PROJECTS_FNAME_ROOT=f'{OUT_DIR}/{CNCF_PROJECTS_FNAME_BASE}'\n",
    "\n",
    "print(f'Jupyter Kernel (venv): {sys.executable}')\n",
    "print(f'Output Location:       {OUT_DIR}  (.json, .jsonl, .csv, .md, ...)')\n",
    "print(f'Output Landscape root: {CNCF_LANDSCAPE_FNAME_ROOT}-*')\n",
    "print(f'Output Projects  root: {CNCF_PROJECTS_FNAME_ROOT}-*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download landscape data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-11-21 16:41:53--  https://landscape.cncf.io/data/items.json\n",
      "Resolving landscape.cncf.io (landscape.cncf.io)... 2600:1f18:2489:8201::c8, 2600:1f18:16e:df02::64, 44.217.161.11, ...\n",
      "Connecting to landscape.cncf.io (landscape.cncf.io)|2600:1f18:2489:8201::c8|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 10194812 (9.7M) [application/json]\n",
      "Saving to: ‘generated/cncf-landscape.json.compact’\n",
      "\n",
      "generated/cncf-land 100%[===================>]   9.72M  4.42MB/s    in 2.2s    \n",
      "\n",
      "2023-11-21 16:41:55 (4.42 MB/s) - ‘generated/cncf-landscape.json.compact’ saved [10194812/10194812]\n",
      "\n",
      "-rw-r--r--  1 matt  staff   9.7M Nov 21 16:41 generated/cncf-landscape.json.compact\n"
     ]
    }
   ],
   "source": [
    "# create directory for generated files\n",
    "!mkdir -p {OUT_DIR}\n",
    "\n",
    "#Get the current landscape json (array) file\n",
    "!wget -O {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact https://landscape.cncf.io/data/items.json\n",
    "!ls -lh {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compact -> human friendly, generate csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes indeed, that's 2+ MB of whitespace!\n",
      "\n",
      "-rw-r--r--  1 matt  staff    12M Nov 21 16:42 generated/cncf-landscape.json\n",
      "-rw-r--r--  1 matt  staff   9.7M Nov 21 16:41 generated/cncf-landscape.json.compact\n"
     ]
    }
   ],
   "source": [
    "!jq . {CNCF_LANDSCAPE_FNAME_ROOT}.json.compact > {CNCF_LANDSCAPE_FNAME_ROOT}.json\n",
    "\n",
    "!echo \"Yes indeed, that's 2+ MB of whitespace!\\n\"\n",
    "!ls -lh {CNCF_LANDSCAPE_FNAME_ROOT}.json* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "# array of JSON --> JSONL\n",
    "jq  -c '.[]'  {CNCF_LANDSCAPE_FNAME_ROOT}.json >  {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter 1200+ cards/entities --> ~180 CNCF Projects --> cncf-projects.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls -lahF {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "wc -l    {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl\n",
    "\n",
    "# landscape | select(CNCF Projects) --> cncf-projects.jsonl\n",
    "jq -c 'select(.relation == \"graduated\" or .relation == \"incubating\" or .relation == \"sandbox\")'  {CNCF_LANDSCAPE_FNAME_ROOT}.jsonl > cncf-projects.jsonl \n",
    "\n",
    "ls -lahF cncf-projects.jsonl\n",
    "wc -l cncf-projects.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame helpers (safe_set_index(), split_org_repo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_rows', 512)\n",
    "pd.set_option('display.max_columns', 512)\n",
    "pd.set_option('display.width', 512)\n",
    "\n",
    "# https://plotly.com/python/pandas-backend\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "def safe_set_index(df:         pd.DataFrame, \n",
    "                   idx_wanted: list[str],\n",
    "                   sort:       bool = True,\n",
    "                   inplace:    bool = True) -> pd.DataFrame:\n",
    "    '''check to see if the index is already set, else, data loss as set_index can be destructive'''\n",
    "    \n",
    "    idx_existing = list(df.index.names)\n",
    "\n",
    "    if idx_wanted == idx_existing:\n",
    "        print(f'\\n*** WARNING: attempt to set index to what it already is thwarted! \\n')\n",
    "    else:\n",
    "        df.set_index(idx_wanted, verify_integrity=True, inplace=inplace)\n",
    "        print(f'\\t Index changed from {idx_existing} --> {list(df.index.names)}') \n",
    "\n",
    "    if sort:\n",
    "        df.sort_index(inplace=inplace)\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_org_repo(df:      pd.DataFrame, \n",
    "                   colname: str,\n",
    "                   drop:    bool = False,\n",
    "                   newcol_org_name:  str = 'org_name',\n",
    "                   newcol_repo_name: str = 'repo_name') -> pd.DataFrame:\n",
    "    '''split_org_repo(df, colname) - org_name/repo_name --> org_name, repo_name'''\n",
    "    \n",
    "    if colname is None:\n",
    "        raise ValueError('split_org_repo: missing colname!')\n",
    "\n",
    "    # https://swdevnotes.com/python/2022/extract-data-from-json-in-pandas-dataframe/\n",
    "    # expand=True returns a dataframe  which we can rename columns on\n",
    "    \n",
    "    df_newcols = df[colname].copy().str.split(pat='/', n=1, expand=True)\n",
    "    df_newcols.rename(columns={0: newcol_org_name, 1: newcol_repo_name}, inplace=True)\n",
    "\n",
    "    if drop:\n",
    "        df.drop(colname, axis=1, inplace=True)\n",
    "\n",
    "    df = pd.concat([df,df_newcols], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre pthe Landscape from cncf-projects.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './cncf-projects.jsonl'\n",
    "\n",
    "if os.path.exists(file_path) and os.path.getsize(file_path) > 0:\n",
    "    df_projects = pd.read_json(file_path, lines=True)\n",
    "else:\n",
    "    print(f\"File {file_path} does not exist or is empty.\")\n",
    "\n",
    "print(df_projects.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cols += { subcategory, repo, org_name, repo_name }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up path to strip out the category (path := category / subcategory)\n",
    "df_projects['subcategory'] = df_projects['path'].str.split('/').str[-1]\n",
    "\n",
    "# https://github.com/theOrg/theRepo --> theOrg/theRepo\n",
    "df_projects['repo'] = df_projects['repo_url'].astype('string').str.removeprefix('https://github.com/')\n",
    "\n",
    "# theOrg/theRepo --> theOrg, theRepo\n",
    "df_projects = split_org_repo(df_projects, 'repo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_projects.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a smaller (cleaner) dataframe to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2=df[['B','D','F']].rename({'B':'X','D':'Y','F':'Z'}, axis=1)\n",
    "df = df_projects.reset_index()\n",
    "df = df[[\n",
    "    'relation',\n",
    "    'category',\n",
    "    'subcategory',\n",
    "    'id',\n",
    "    'name',\n",
    "    'flatName',\n",
    "    'repo',\n",
    "    'repo_name',\n",
    "    'org_name',\n",
    "    'contributorsCount',\n",
    "    'commitsThisYear',\n",
    "    'stars',\n",
    "    'github_data',\n",
    "    'extra',\n",
    "    'industries',\n",
    "    'headquarters',\n",
    "    'image_data']].copy()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'before nulls removed: {df.shape}')\n",
    "df = df.dropna().copy()\n",
    "print(f'after nulls removed: {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['category'].drop_duplicates().tolist()\n",
    "print(f'CATEGORIES\\n{categories}\\n')\n",
    "\n",
    "subcategories = df['subcategory'].drop_duplicates().tolist()\n",
    "print(f'SUBCATEGORIES\\n{subcategories}\\n')\n",
    "\n",
    "org_names = df['org_name'].drop_duplicates().tolist()\n",
    "print(f'ORG_NAMES\\n{org_names}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index --> ['relation', 'category', 'subcategory', 'id']\n",
    "\n",
    "df_numeric = df.select_dtypes(include=['int64', 'float64'])\n",
    "df_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation at level 0\n",
    "df_level0_relation = df_numeric.groupby(level=0).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 1\n",
    "df_level1_category = df_numeric.groupby(level=1).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 2\n",
    "df_level2_subcategory = df_numeric.groupby(level=2).sum()  # or .mean(), .count(), etc.\n",
    "\n",
    "# Aggregation at level 3\n",
    "# df_level3 = df.groupby(level=3).sum()  # or .mean(), .count(), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Aggregations at df_level0_relation\\n\\n{df_level0_relation}\\n')\n",
    "print(f'Aggregations at df_level1_category\\n\\n{df_level1_category}\\n')\n",
    "print(f'Aggregations at df_level2_subcategory\\n\\n{df_level2_subcategory}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'df.index.names: {df.index.names}\\n\\n')\n",
    "# #print(f'df.index.levels: {df.index.levels}\\n\\n')\n",
    "\n",
    "# for level in df.index.levels:\n",
    "#     print(f'level: {level}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sunbursts and Treemaps for { Contributors Count, Commits this year, Stars }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light to Dark Transition:\n",
    "# color_discrete_map={'sandbox': '#ADD8E6', 'incubating': '#87CEEB', 'graduated': '#000080'}\n",
    "#\n",
    "# sandbox: Light Blue (#ADD8E6)\n",
    "# incubating: Medium Blue (#87CEEB)\n",
    "# graduated: Dark Blue (#000080)\n",
    "\n",
    "# Warm to Cool Transition:\n",
    "# color_discrete_map={'sandbox': '#FFA500', 'incubating': '#FFD700', 'graduated': '#008000'}\n",
    "#\n",
    "# sandbox: Orange (#FFA500)\n",
    "# incubating: Yellow (#FFD700)\n",
    "# graduated: Green (#008000)\n",
    "\n",
    "# Warm to Cool 2 Transition:\n",
    "# color_discrete_map={'sandbox': '#FFD700', 'incubating': '#87CEEB', 'graduated': '#008000'}\n",
    "#\n",
    "# sandbox: Yellow (#FFD700)\n",
    "# incubating: Medium Blue (#87CEEB)\n",
    "# graduated: Green (#008000)\n",
    "\n",
    "\n",
    "\n",
    "def create_figure(plotly_func,\n",
    "                  df,\n",
    "                  values=None,\n",
    "                  height: int = 1200,\n",
    "                  width: int = 1200,\n",
    "                  title: str = 'Missing Title',\n",
    "                  path=['relation', 'category', 'subcategory', 'id'],\n",
    "                  color='relation',\n",
    "                  color_discrete_map={'sandbox': '#FFD700', 'incubating': '#87CEEB', 'graduated': '#008000'},\n",
    "                  branchvalues: str = None) -> go.Figure:\n",
    "    \n",
    "    fig = plotly_func(data_frame=df,\n",
    "                      values=values,\n",
    "                      height=height,\n",
    "                      width=width,\n",
    "                      title=title,\n",
    "                      path=path,\n",
    "                      color=color,\n",
    "                      color_discrete_map=color_discrete_map,\n",
    "                      branchvalues='total')\n",
    "    return fig\n",
    "\n",
    "\n",
    "def create_sunburst(df, **kwargs) -> go.Figure:\n",
    "    return create_figure(px.sunburst, df, **kwargs)\n",
    "\n",
    "\n",
    "def create_treemap(df, **kwargs) -> go.Figure:\n",
    "    return create_figure(px.treemap, df, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Figures (Sunbursts, Treemaps) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make index columns accessible for charting as normal columns\n",
    "df_reset = df.reset_index()\n",
    "\n",
    "figs = {}\n",
    "\n",
    "sunb_contributorsCount = create_sunburst(df_reset, values='contributorsCount', title='sunburst: 🪴 Contributor Count (NOT UNIQUE ACROSS PROJECTS!) 🪴')\n",
    "sunb_commitsThisYear   = create_sunburst(df_reset, values='commitsThisYear',   title='sunburst: 📄 Commits This Year 📄')\n",
    "sunb_stars             = create_sunburst(df_reset, values='stars',             title='sunburst: ⭐ Stars ⭐')\n",
    "\n",
    "tree_contributorsCount = create_treemap(df_reset, values='contributorsCount',  title='treemap: 🪴 Contributor Count (NOT UNIQUE ACROSS PROJECTS!) 🪴')\n",
    "tree_commitsThisYear   = create_treemap(df_reset, values='commitsThisYear',    title='treemap: 📄 Commits This Year 📄')\n",
    "tree_stars             = create_treemap(df_reset, values='stars',              title='treemap: ⭐ Stars ⭐')\n",
    "\n",
    "figs['sunb_contributorsCount'] = sunb_contributorsCount\n",
    "figs['sunb_commitsThisYear']   = sunb_commitsThisYear\n",
    "figs['sunb_stars']             = sunb_stars\n",
    "\n",
    "figs['tree_contributorsCount'] = tree_contributorsCount\n",
    "figs['tree_commitsThisYear']   = tree_commitsThisYear\n",
    "figs['tree_stars']             = tree_stars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save figures as .svg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, fig in figs.items():\n",
    "    file_name = f'fig_{key}.svg'\n",
    "    fig.write_image(file_name, format='svg')\n",
    "\n",
    "    # Emit raw markdown for image description\n",
    "    markdown = f\"![Image description]({file_name})\"\n",
    "    print(f\"```{markdown}```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save --> files so viewable in markdown cells\n",
    "#\n",
    "# ![Image description](path_to_image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Sunbursts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_stars'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_contributorsCount'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['sunb_commitsThisYear'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Treemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_contributorsCount'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_commitsThisYear'].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs['tree_contributorsCount'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### images for github rendering\n",
    "\n",
    "![Image description](fig_sunb_contributorsCount.svg)\n",
    "![Image description](fig_sunb_commitsThisYear.svg)\n",
    "![Image description](fig_sunb_stars.svg)\n",
    "![Image description](fig_tree_contributorsCount.svg)\n",
    "![Image description](fig_tree_commitsThisYear.svg)\n",
    "![Image description](fig_tree_stars.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Per TAG views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())\n",
    "print(f'index.names: {df.index.names}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_tag = df.safe_set_index("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug - just Observability TAG Projects\n",
    "#repos_by_relation = df.query(\"`category` == 'Observability and Analysis'\")[['relation','repo', 'name']].copy()\n",
    "\n",
    "repos_by_relation = df[['relation','repo', 'name']].copy()\n",
    "repos_by_relation.groupby('relation')['repo'].agg(lambda x: list(x)).to_dict()\n",
    "\n",
    "safe_set_index(repos_by_relation, idx_wanted=['relation', 'name'])\n",
    "repos_by_relation.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graduated_single_repos  = repos_by_relation.loc['graduated', :]['repo'].tolist()\n",
    "incubating_single_repos = repos_by_relation.loc['incubating', :]['repo'].tolist()\n",
    "sandbox_single_repos    = repos_by_relation.loc['sandbox', :]['repo'].tolist()\n",
    "\n",
    "display(graduated_single_repos, incubating_single_repos, sandbox_single_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch project release data from GitHub API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from datetime import datetime, timezone\n",
    "from github import Github, GithubException\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 150)\n",
    "\n",
    "def fetch_repo_data(token: str, \n",
    "                    repo_list: List[str], \n",
    "                    since: datetime=None, \n",
    "                    json_file: str=None, \n",
    "                    csv_file: str=None, \n",
    "                    state_file: str=None) -> pd.DataFrame:\n",
    "\n",
    "    # Initialize DataFrame\n",
    "    # df = pd.DataFrame(columns=[\n",
    "    #     'repo_name', 'release_name', 'release_date', \n",
    "    #     'language', 'release_notes'\n",
    "    # ])\n",
    "\n",
    "    df = pd.DataFrame(columns=['repo_name', 'release_name', 'release_date', 'language'])\n",
    "\n",
    "    # Initialize GitHub client\n",
    "    g = Github(token)\n",
    "\n",
    "    # Initialize loop state\n",
    "    if state_file:\n",
    "        try:\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            state = {'i': 0, 'repos_done': []}\n",
    "    else:\n",
    "        state = {'i': 0, 'repos_done': []}\n",
    "\n",
    "    # Loop over repositories\n",
    "    while state['i'] < len(repo_list):\n",
    "        repo_str = repo_list[state['i']]\n",
    "\n",
    "        if repo_str in state['repos_done']:\n",
    "            print(f\"Skipping: {repo_str}\")\n",
    "            state['i'] += 1\n",
    "            continue\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                repo = g.get_repo(repo_str)\n",
    "                break\n",
    "            except GithubException as e:\n",
    "                if e.status == 404:\n",
    "                    print(f\"Repository {repo_str} not found\")\n",
    "                    break\n",
    "                elif e.status == 429:\n",
    "                    print(f\"Rate limit exceeded, waiting for {e.headers['Retry-After']} seconds...\")\n",
    "                    time.sleep(int(e.headers['Retry-After']))\n",
    "                else:\n",
    "                    print(f\"Error getting repository {repo_str}: {e}\")\n",
    "                    break\n",
    "\n",
    "        if not repo:\n",
    "            state['i'] += 1\n",
    "            continue\n",
    "\n",
    "        #\n",
    "        # Get all releases\n",
    "        #\n",
    "        releases = repo.get_releases()\n",
    "        language = repo.language\n",
    "\n",
    "        for release in releases:\n",
    "            if since is None or release.created_at >= since:\n",
    "                df = pd.concat([df, pd.DataFrame({\n",
    "                    'repo_name': [repo_str],\n",
    "                    'release_name': [release.title],\n",
    "                    'release_date': [str(release.published_at)],\n",
    "                    'language': [language],\n",
    "                    #'release_notes': [release.body]\n",
    "                })])\n",
    "                print(f\"Added {release.published_at}, {repo_str}::{release.title}  \")\n",
    "\n",
    "        # Save state\n",
    "        if state_file:\n",
    "            state['repos_done'].append(repo_str)\n",
    "            with open(state_file, 'w') as f:\n",
    "                json.dump(state, f, indent=4)\n",
    "\n",
    "        state['i'] += 1\n",
    "\n",
    "    #print (releases)\n",
    "    \n",
    "    # Save as CSV\n",
    "    if csv_file:\n",
    "        df.to_csv(csv_file, index=False)\n",
    "\n",
    "    # Save as JSON\n",
    "    if json_file:\n",
    "        df.to_json(json_file, orient='records', lines=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_one(token, since_date, level, repos):\n",
    "\n",
    "    json_file=f'out/{level}-github-releases.json' \n",
    "    csv_file=f'out/{level}-github-releases.csv'\n",
    "    state_file=f'out/.nukeme_state_file_{level}'\n",
    "    \n",
    "    print(f\"Fetching {len(repos)} repositories for {level} projects\")\n",
    "    \n",
    "    releases = fetch_repo_data( token, \n",
    "                                repos, \n",
    "                                since=since_date,\n",
    "                                json_file=json_file,\n",
    "                                csv_file=csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = os.environ['GITHUB_TOKEN']\n",
    "since_date = datetime(2022, 11, 7, tzinfo=timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-graduated', graduated_single_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-incubating', incubating_single_repos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_one(token, since_date, 'cncf-sandbox', sandbox_single_repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Releases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipympl\n",
    "\n",
    "%matplotlib inline\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# def plot_releases_timeline(releases: pd.DataFrame):\n",
    "#     fig = px.timeline(releases, x_start=\"release_date\", x_end=\"release_date\", y=\"repo_name\", color=\"language\", title=\"GitHub Releases Timeline\")\n",
    "#     fig.update_yaxes(autorange=\"reversed\")\n",
    "#     fig.show()\n",
    "\n",
    "# def plot_releases_scatter_simple(releases: pd.DataFrame):\n",
    "#     # Filter releases by year\n",
    "#     releases_2023 = releases[releases['release_date'].dt.year == 2023]\n",
    "\n",
    "#     # Create scatter plot\n",
    "#     fig = px.scatter(releases_2023, x=\"release_date\", y=\"repo_name\", color=\"language\")\n",
    "#     fig.update_yaxes(autorange=\"reversed\")\n",
    "#     fig.show()\n",
    "\n",
    "def plot_releases_scatter(releases: pd.DataFrame, title: str=None):\n",
    "    # # Filter releases by year\n",
    "    # releases_2023 = releases[releases['release_date'].dt.year == 2023]\n",
    "\n",
    "    if title is None:\n",
    "        title = \"GitHub Releases Timeline\"\n",
    "\n",
    "    # Group releases by organization\n",
    "    releases['organization'] = releases['repo_name'].apply(lambda x: x.split('/')[0])\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = px.scatter(releases, x=\"release_date\", y=\"repo_name\", color=\"organization\", symbol=\"language\", title=\"Project Releases\")\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    \n",
    "    fig.update_layout(showlegend=True,\n",
    "                      autosize=True,\n",
    "                      width=1000)\n",
    "                    #   height=2500,\n",
    "                    #   )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def json_to_csv(json_file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Load a JSON file into a pandas DataFrame and save it as a CSV file with the same name.\n",
    "    \"\"\"\n",
    "    df = pd.read_json(json_file_path, lines=True)\n",
    "    \n",
    "    csv_file_path = os.path.splitext(json_file_path)[0] + '.csv'\n",
    "    df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in ['graduated', 'incubating', 'sandbox']:\n",
    "    json_to_csv(f'out/cncf-{level}-github-releases.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_releases_from_csv(csv_file: str, title: str) -> None:\n",
    "    csv_file =f'out/cncf-{level}-github-releases.csv'\n",
    "    if os.path.exists(csv_file):\n",
    "        df_releases = pd.read_csv(csv_file)\n",
    "        df_releases.release_date = pd.to_datetime(df_releases.release_date)\n",
    "        \n",
    "        plot_releases_scatter(df_releases, title)\n",
    "    else:\n",
    "        print(f\"CSV file {csv_file} not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "for level in ['graduated', 'incubating', 'sandbox']:\n",
    "    plot_releases_from_csv(f'out/cncf-{level}-github-releases.csv', f'Releases: {level}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# plot_releases_scatter(f'out/cncf-all-github-releases.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "releases_by_repo = releases[['repo_name', 'release_date']].groupby('repo_name').count()\n",
    "releases_by_repo.to_csv('cncf_releases_by_repo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
